{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load seminar.py\n",
    "# %%\n",
    "import bokeh.plotting as pl\n",
    "import bokeh.models as bm\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.io import output_notebook\n",
    "from sklearn.decomposition import PCA\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import numpy as np\n",
    "# %%\n",
    "data = list(open(\n",
    "    'yandexdataschool_nlp_course\\week01_word_embedding\\quora.txt', encoding='utf-8'))\n",
    "# data = list(open('quora.txt',encoding='utf-8'))\n",
    "print(data[50])\n",
    "\n",
    "# %% [markdown]\n",
    "# 使用nltk进行处理文本。因为文本里面含有大量特殊符号标点，引用nltk会使得处理变得简单\n",
    "\n",
    "# %%\n",
    "# Tokenize a text into a sequence of alphabetic and non-alphabetic characters\n",
    "tokenizer = WordPunctTokenizer()  # 实例化对象\n",
    "print(tokenizer.tokenize(data[50]))\n",
    "\n",
    "# %%\n",
    "# lowercase everything and extract tokens with tokenizer\n",
    "# data_tok should be a list of lists of tokens for each line in data\n",
    "data_tok = [tokenizer.tokenize(i.lower()) for i in data]\n",
    "\n",
    "# %%\n",
    "print(len(data))\n",
    "print(len(data_tok))\n",
    "type(data[1])  # str. data里面的内容是str\n",
    "\n",
    "# %%\n",
    "print(data_tok[530])\n",
    "type(data_tok[2])  # list. 说明这是list的嵌套\n",
    "\n",
    "# %%\n",
    "# require that every element in data_tok is list or tuple\n",
    "assert all(isinstance(row, (list, tuple))\n",
    "           for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
    "assert all(all(isinstance(tok, str) for tok in row)\n",
    "           for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
    "\n",
    "\n",
    "def is_latin(tok): return all('a' <= x.lower() <= 'z' for x in tok)\n",
    "\n",
    "\n",
    "assert all(map(lambda l: not is_latin(l) or l.islower(), map(\n",
    "    ' '.join, data_tok))), \"please make sure to lowercase the data\"\n",
    "\n",
    "# %%\n",
    "# print([' '.join(row) for row in data_tok[::100000]]) # data_tok数据量庞大，不建议执行,这是将序列转为文本\n",
    "\n",
    "\n",
    "#%% [markdown]\n",
    "# # Word2Vec 的一些简单实用说明\n",
    "# https://rare-technologies.com/word2vec-tutorial/ <br/>\n",
    "# https://blog.csdn.net/qq_19707521/article/details/79169826 <br/>\n",
    "# # 训练模型\n",
    "# * 利用```gensim.models.Word2Vec(sentences)```建立词向量模型：\n",
    "#       经历了三个步骤：<br/>\n",
    "#           建立一个空的模型对象<br/>\n",
    "#           遍历一次语料建立词典<br/>\n",
    "#           第二次遍历语料库建立神经网络模型<br/>\n",
    "#       可以分别通过执行```model=gensim.models.Word2Vec()```<br/>\n",
    "#       ```model.build_vocab(sentences)```<br/>\n",
    "#       ```model.train(sentences)``` <br/>\n",
    "# # Preparing the Input\n",
    "# Starting from the beginning, gensim’s word2vec expects a sequence of sentences as its input. Each sentence a list of words (utf8 strings):\n",
    "# \n",
    "#%%\n",
    "import sys\n",
    "sentences = [['first','sentence'],['second','sentence']]\n",
    "# train word2vec on the two sentences\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "# print(model.get_vector('first')) # 该model类型没有相应属性，原因是类型与下文model不同，原因未知\n",
    "type(model) # gensim.models.word2vec.Word2Vec\n",
    "type(model.wv.__getitem__('first')) # numpy.ndarray\n",
    "print(model.wv.__getitem__('first'))\n",
    "\n",
    "#%% [markdown]\n",
    "# gensim only requires that the input must provide sentences sequentially, when iterated over\n",
    "\n",
    "\n",
    "\n",
    "#%%\n",
    "# 将data_tok转为模型\n",
    "model = Word2Vec(data_tok,\n",
    "                 size=32,  # embedding vector size\n",
    "                 min_count=5,  # consider words that occured at least 5 times\n",
    "                 window=5).wv  # define context as a 5-word window around the target word\n",
    "#%%\n",
    "# 注意这里的model的类型与上文的model的类型不同，原因未知\n",
    "type(model) # gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "#%% [markdown]\n",
    "# # storing and loading models\n",
    "#%%\n",
    "# model.save_word2vec_format('yandexdataschool_nlp_course/week01_word_embedding/mymodel')\n",
    "#%%\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('yandexdataschool_nlp_course/week01_word_embedding/mymodel')\n",
    "new_model = KeyedVectors.load_word2vec_format('yandexdataschool_nlp_course/week01_word_embedding/mymodel')\n",
    "#%%\n",
    "print(type(model))\n",
    "print(type(new_model))\n",
    "# %%\n",
    "# get word vectors\n",
    "print(model.get_vector('anything'))\n",
    "print(new_model.get_vector('anything')) # this is a test\n",
    "print(model.__getitem__('anything'))\n",
    "\n",
    "# %%\n",
    "# query similar words directly.\n",
    "print(model.most_similar('you'))\n",
    "\n",
    "# %% [markdown]\n",
    "# # Using pre-trained model\n",
    "# to process a large data\n",
    "# %%\n",
    "# 这里的20-newsgroups是一个数据集\n",
    "# 数据集与模型网站https://github.com/RaRe-Technologies/gensim-data\n",
    "# model = api.load('20-newsgroups')\n",
    "# %%\n",
    "# glove-wiki-gigaword-50是一个模型\n",
    "# model = api.load('glove-wiki-gigaword-50')\n",
    "#%%\n",
    "# model.save_word2vec_format('yandexdataschool_nlp_course/week01_word_embedding/glove-wiki-gigaword-50')\n",
    "#%%\n",
    "model = KeyedVectors.load_word2vec_format('yandexdataschool_nlp_course/week01_word_embedding/glove-wiki-gigaword-50')\n",
    "#%%\n",
    "# gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "type(model)\n",
    "# %%\n",
    "model.most_similar('man')\n",
    "\n",
    "#%%\n",
    "model.most_similar(positive=['coder', 'money'], negative=['brain'])\n",
    "#%%\n",
    "print(model.similarity('mail','e-mail'))\n",
    "print(model.most_similar(positive=['queen','king'], negative=['man']))\n",
    "\n",
    "# %% [markdown]\n",
    "# # Visualizing word vectors\n",
    "# 词的向量维数通常是在20维以上，人类只能识别三维及其以下，需要利用dimensionality reduction进行降维\n",
    "# %%\n",
    "words = sorted(model.vocab.keys(),\n",
    "               key=lambda word: model.vocab[word].count,\n",
    "               reverse=True)[:1000]\n",
    "#%%\n",
    "print(words[::100])\n",
    "# %%\n",
    "# for each words, compute it's vector\n",
    "word_vectors = np.array([model.get_vector(i) for i in words])\n",
    "len(words)\n",
    "# %%\n",
    "# 这里是判断上述代码是否运行正确\n",
    "assert isinstance(word_vectors, np.ndarray)\n",
    "assert word_vectors.shape == (len(words), 50)\n",
    "assert np.isfinite(word_vectors).all()\n",
    "print(type(word_vectors)) # numpy.ndarray.shape return a tuple in which the first value is the len of word_vectors and the second is the len of element\n",
    "print(word_vectors.shape) # type(word_vectors.shape) == tuple, (1000,50)\n",
    "print(len(word_vectors[0]))\n",
    "# %% [markdown]\n",
    "# # Linear projection:PCA\n",
    "# map wordvectors onto 2d plane with PCA. Use good old sklearn api (fit,transform)\n",
    "# after that, normalize vectors to make sure they have zero mean and unit variance\n",
    "#%%\n",
    "word_vectors_pca = PCA(n_components=2).fit_transform(word_vectors)\n",
    "#%%\n",
    "#%%\n",
    "# numpy.naddrray\n",
    "type(word_vectors_pca)\n",
    "#%%\n",
    "print(word_vectors_pca.mean())\n",
    "\n",
    "# %%\n",
    "assert word_vectors_pca.shape == (\n",
    "    len(word_vectors), 2), \"there must be a 2d vector for each word\"\n",
    "assert max(abs(word_vectors_pca.mean(0))\n",
    "           ) < 1e-5, \"points must be zero-centered\"\n",
    "\n",
    "# don't know why\n",
    "print(max(abs(1.0-word_vectors_pca.std(0)))) # 0.2002182\n",
    "# assert max(abs(1.0-word_vectors_pca.std(0))\n",
    "#           ) < 1e-5, \"points must have unit variance\"\n",
    "\n",
    "# %% [markdown]\n",
    "# # Let's draw it!\n",
    "# %%\n",
    "# 这里注意区分bokeh.models和bokeh.model的区别\n",
    "# 关于bokeh的学习请参考我的同文件夹下的另一篇代码文章bokeh_learn.py\n",
    "# from bokeh.io import output_notebook\n",
    "# import bokeh.plotting as pl\n",
    "# import bokeh.models as bm\n",
    "output_notebook()\n",
    "\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    if isinstance(color, str): color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: pl.show(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "# %%\n",
    "draw_vectors(word_vectors_pca[:, 0], word_vectors_pca[:, 1], token=words)\n",
    "\n",
    "# %%\n",
    "word_tsne = TSNE(verbose=True).fit_transform(word_vectors)\n",
    "# %%\n",
    "draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=words)\n",
    "\n",
    "#%% [markdown]\n",
    "# # Visualizing phrases\n",
    "# Word embeddings can also be used to represent short phrases. The simplest way is to take an average of vectors for all tokens in the phrase with some weights.<br/>\n",
    "# This trick is useful to identify what data are you working with: find if there are any outliers, clusters or other artefacts.<br/>\n",
    "#%%\n",
    "def get_phrase_embedding(phrase):\n",
    "    \"\"\"\n",
    "    Convert phrase to a vector by aggregating it's word embeddings. See description above.\n",
    "    \"\"\"\n",
    "    # 1. lowercase phrase\n",
    "    # 2. tokenize phrase\n",
    "    # 3. average word vectors for all words in tokenized phrase\n",
    "    # skip words that are not in model's vocabulary\n",
    "    # if all words are missing from vocabulary, return zeros\n",
    "    data = list(phrase)\n",
    "    data_tok = [tokenizer.tokenize(i.lower()) for i in data]\n",
    "    model = Word2Vec(data_tok,\n",
    "                    size=32,  # embedding vector size\n",
    "                    min_count=5,  # consider words that occured at least 5 times\n",
    "                    window=5).wv  # define context as a 5-word window around the target word\n",
    "    words = sorted(model.vocab.keys(),\n",
    "               key=lambda word: model.vocab[word].count,\n",
    "               reverse=True)\n",
    "    if(len(data_tok)):\n",
    "        vector = np.array([model.get_vector(i) for i in words])\n",
    "    else:\n",
    "        vector = np.zeros([model.vector_size], dtype='float32')\n",
    "    return vector\n",
    "\n",
    "#%%\n",
    "vector = get_phrase_embedding(\"I'm very sure. This never happened to me before...\")\n",
    "print(type(vector))\n",
    "print(vector)\n",
    "#%%\n",
    "assert np.allclose(vector[::10],\n",
    "                   np.array([ 0.31807372, -0.02558171,  0.0933293 , -0.1002182 , -1.0278689 ,\n",
    "                             -0.16621883,  0.05083408,  0.17989802,  1.3701859 ,  0.08655966],\n",
    "                              dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch for Deeplearning",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
