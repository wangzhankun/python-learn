{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow实现skip-gram\n",
    "原文作者天雨粟，文章来源：\n",
    "[一文详解 Word2vec 之 Skip-Gram 模型（实现篇）](https://www.leiphone.com/news/201706/QprrvzsrZCl4S2lw.html)\n",
    "<br/>\n",
    "文章主要有以下四部分构成：\n",
    "* 数据预处理\n",
    "* 训练样本构建\n",
    "* 模型构建\n",
    "* 模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "预处理过程主要包括：\n",
    "* 替换文本中特殊符号并去除低频词\n",
    "* 对文本分词\n",
    "* 构建语料\n",
    "* 单词映射表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, freq=5):\n",
    "    '''\n",
    "    对文本进行预处理\n",
    "    text: 文本数据\n",
    "    freq: 词频阈值\n",
    "    '''\n",
    "    # 对文本中符号进行替换\n",
    "    text = text.lower()\n",
    "    text = text.replace('.',' <PERIOD> ')\n",
    "    text = text.replace(',', ' <COMMA> ')\n",
    "    text = text.replace('\"', ' <QUOTATION_MARK> ')\n",
    "    text = text.replace(';', ' <SEMICOLON> ')\n",
    "    text = text.replace('!', ' <EXCLAMATION_MARK> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace('(', ' <LEFT_PAREN> ')\n",
    "    text = text.replace(')', ' <RIGHT_PAREN> ')\n",
    "    text = text.replace('--', ' <HYPHENS> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace(':', ' <COLON> ')\n",
    "    words = text.split()\n",
    "    \n",
    "    # 删除低频词汇，减少噪声影响\n",
    "    word_counts = Counter(words)\n",
    "    trimmed_words = [word for word in words if word_counts[word] > freq]\n",
    "    \n",
    "    return trimmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the']\n"
     ]
    }
   ],
   "source": [
    "# 清洗文本并分词\n",
    "words = preprocess(text)\n",
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建映射表\n",
    "vocab = set(words)\n",
    "vocab_to_int = {w:c for c,w in enumerate(vocab)}\n",
    "int_to_vocab = {c:w for c,w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 16680599\n",
      "total unique words 63641\n"
     ]
    }
   ],
   "source": [
    "print(\"total words: {}\".format(len(words)))\n",
    "print(\"total unique words {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将文本转为数字\n",
    "# 这里是将频次当做单词的数值表示，可能某些单词具有相同的频次从而获得\n",
    "# 相同的数值表示\n",
    "int_words = [vocab_to_int[w] for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63504, 49115, 38041, 55048, 11374, 51440, 705, 11778, 56110, 25834, 63004, 45544, 59620, 59349, 13082, 32916, 18361, 51440, 32916, 195, 34650, 17408, 32916, 7616, 22565, 51440, 32916, 34842, 34650, 26248, 32916, 11374, 35781, 35783, 56110, 62923, 55048, 16536, 51233, 59877, 62669, 45236, 59470, 10109, 56110, 60059, 23767, 59877, 62204, 32916]\n"
     ]
    }
   ],
   "source": [
    "print(int_words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 采样\n",
    "严格意义上讲，这里根本不是采样。这里仅仅就是对高频词汇进行了删除。\n",
    "<br/>\n",
    "对停用词进行采样，例如“the”，“of”，“for”等单词进行剔除。\n",
    "采用公式：\n",
    "$$\n",
    "P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i}}\n",
    "\\\\\n",
    "t: 阈值参数，一般取1e-3 至 1e-5\n",
    "\\\\\n",
    "f(w_i): 单词w_i在整个数据集中出现的频次\n",
    "\\\\\n",
    "P(w_i)：单词被删除的概率\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1e-5\n",
    "threshold = 0.8 # 剔除概率阈值\n",
    "# 统计单词出现频次\n",
    "int_word_counts = Counter(int_words) # 每个单词的对应的数量\n",
    "total_count = len(int_words) # 所有单词的总数\n",
    "# 计算频次\n",
    "word_freqs = {w:c/total_count for w,c in int_word_counts.items()}\n",
    "# 计算被删除的概率\n",
    "prob_drop = {w: 1-np.sqrt(t / word_freqs[w]) for w in int_word_counts}\n",
    "# 对单词进行采样\n",
    "train_words  = [w for w in int_words if prob_drop[w] < threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6925252"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造batch\n",
    "Skip-Gram模型是通过输入词来预测上下文。\n",
    "<br/>\n",
    "对于一个给定词，离它越近的词可能与它越相关，离它越远的词越不相关，这里我们设置窗口大小为5，对于每个训练单词，我们还会在\\[1:5\\]之间随机生成一个整数R，用R作为我们最终选择output word的窗口大小。这里之所以多加了一步随机数的窗口重新选择步骤，是为了能够让模型更聚焦于当前input word的邻近词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(words, idx, window_size=5):\n",
    "    '''\n",
    "    获得input word的上下文单词列表\n",
    "    words: 单词列表\n",
    "    idx: input words的索引号\n",
    "    window_size: 窗口大小\n",
    "    '''\n",
    "    target_window = np.random.randint(1, window_size+1) # 随机生成目标窗口大小\n",
    "    # 这里要考虑input word前面单词不够的情况\n",
    "    start_point = idx - target_window if (idx - target_window) > 0 else 0\n",
    "    end_point = idx + target_window\n",
    "    targets = set(words[start_point:idx] + words[idx+1:end_point+1])\n",
    "    return list(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size = 5):\n",
    "    '''\n",
    "    构造一个获取batch的生成器\n",
    "    '''\n",
    "    # 有多少个小批量，每个批量的单词数是batch_size\n",
    "    n_batches = len(words) // batch_size\n",
    "    \n",
    "    # 仅取full batches\n",
    "    words = words[:n_batches * batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x,y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for i in range(len(batch)):\n",
    "            # input word\n",
    "            batch_x = batch[i]\n",
    "            # output word\n",
    "            batch_y = get_targets(batch, i, window_size)\n",
    "            # 由于一个input word会对应多个output word，因此需要统一长度\n",
    "            x.extend([batch_x] * len(batch_y))\n",
    "            y.extend(batch_y)\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建网络\n",
    "该部分主要包括：\n",
    "* 输入层\n",
    "* Embedding\n",
    "* Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    inputs = tf.placeholder(tf.int32, shape=[None], name='inputs')\n",
    "    labels = tf.placeholder(tf.int32, shape=[None, None], name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "嵌入矩阵的形状为$vocab \\times hidden_units_size$\n",
    "<br/>\n",
    "TensorFlow中的tf.nn.embedding_lookup函数可以实现lookup的计算方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(int_to_vocab)\n",
    "embedding_size = 200 # 嵌入维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    # 嵌入层权重矩阵\n",
    "    embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size]))\n",
    "    # 实现lookup\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Sampling\n",
    "负采样主要解决梯度下降计算速度慢问题。\n",
    "<br/>\n",
    "TensorFlow中的tf.nn.sampled_softmax_loss会在softmax层上进行采样计算损失，计算出的loss要比full softmax loss低"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 负例数量\n",
    "n_sampled = 100\n",
    "\n",
    "with train_graph.as_default():\n",
    "    softmax_w = tf.Variable(tf.truncated_normal([vocab_size, embedding_size], stddev = 0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(vocab_size))\n",
    "    \n",
    "    # 计算negative sampling下的损失\n",
    "    loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, labels, embed, n_sampled, vocab_size)\n",
    "    \n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证\n",
    "为了更加直观的看到我们训练的结果，我们将查看训练出的相近语义的词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-f85ace3c18a8>:15: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "with train_graph.as_default():\n",
    "    # 随机挑选一些单词\n",
    "    valid_size = 16\n",
    "    valid_window = 100\n",
    "    # 从不同位置各选8各单词\n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size // 2))\n",
    "    valid_examples = np.append(valid_examples, \n",
    "                               random.sample(range(1000, 1000+valid_window), valid_size//2))\n",
    "    \n",
    "    valid_size = len(valid_examples)\n",
    "    # 验证单词集\n",
    "    valid_dataset = tf.constant(valid_examples, dtype = tf.int32)\n",
    "    \n",
    "    # 计算每个单词向量的模并进行单位化\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "    normalized_embedding = embedding / norm\n",
    "    # 查找验证单词的词向量\n",
    "    valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "    # 计算余弦相似度\n",
    "    similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Iteration: 100 Avg. Training loss: 3.4194 0.1995 sec/batch\n",
      "Epoch 1/10 Iteration: 200 Avg. Training loss: 2.8638 0.1862 sec/batch\n",
      "Epoch 1/10 Iteration: 300 Avg. Training loss: 2.4914 0.1872 sec/batch\n",
      "Epoch 1/10 Iteration: 400 Avg. Training loss: 2.6493 0.2007 sec/batch\n",
      "Epoch 1/10 Iteration: 500 Avg. Training loss: 2.7031 0.1949 sec/batch\n",
      "Epoch 1/10 Iteration: 600 Avg. Training loss: 2.8114 0.1935 sec/batch\n",
      "Epoch 1/10 Iteration: 700 Avg. Training loss: 2.9569 0.1808 sec/batch\n",
      "Epoch 1/10 Iteration: 800 Avg. Training loss: 3.0955 0.1818 sec/batch\n",
      "Epoch 1/10 Iteration: 900 Avg. Training loss: 3.3665 0.1802 sec/batch\n",
      "Epoch 1/10 Iteration: 1000 Avg. Training loss: 3.5944 0.1796 sec/batch\n",
      "Nearest to [outlived]: dl, duomo, formalization, destroyer, mug, infinitesimal, rodger, abydos,\n",
      "Nearest to [rated]: psychomotor, obj, worcester, stopwatch, offroad, dragging, wavelet, hogshead,\n",
      "Nearest to [dragster]: foonly, functioned, maradns, cnts, clemency, icosahedron, wold, bonds,\n",
      "Nearest to [simplicity]: passenger, relocate, presumed, realplayer, government, immortals, payton, carpio,\n",
      "Nearest to [amongst]: longitudinal, satirize, friends, assur, coloured, halt, webring, pinball,\n",
      "Nearest to [dozenal]: ukrainian, regrets, cascades, sens, pediatrics, decisive, anchorite, considerations,\n",
      "Nearest to [enharmonic]: salvadoran, redefinition, creamy, sortie, tacticus, marching, maharishi, demonstrably,\n",
      "Nearest to [ascetical]: roared, desertion, boulez, perceval, colman, operates, cmm, combats,\n",
      "Nearest to [staffordshire]: lecturer, crossley, except, canadians, barbershop, appeasing, tampico, kyo,\n",
      "Nearest to [authenticity]: chappell, curtiz, mistreated, quarterly, proctor, businessweek, cassel, basilan,\n",
      "Nearest to [treadmill]: unpleasant, lingering, superheroes, elmendorf, disiyyah, glimpse, multi, abadan,\n",
      "Nearest to [wp]: cutters, webcast, hanssen, ii, erdrich, sheath, lazzeri, omits,\n",
      "Nearest to [syndicalist]: ocampo, hairs, nihilistic, mclaughlin, topographic, kpa, crickets, earthy,\n",
      "Nearest to [geertz]: miming, thabit, rei, thalassemia, revolutionists, perry, socket, minor,\n",
      "Nearest to [caliber]: aoraki, magician, duff, barris, dazzling, macdowell, salome, exciting,\n",
      "Nearest to [hanoverians]: taupin, asimo, plano, akhmad, attractive, bitumen, unsportsmanlike, bukit,\n",
      "Epoch 1/10 Iteration: 1100 Avg. Training loss: 2.9676 0.1827 sec/batch\n",
      "Epoch 1/10 Iteration: 1200 Avg. Training loss: 3.5670 0.1799 sec/batch\n",
      "Epoch 1/10 Iteration: 1300 Avg. Training loss: 3.4183 0.2058 sec/batch\n",
      "Epoch 1/10 Iteration: 1400 Avg. Training loss: 3.6969 0.1971 sec/batch\n",
      "Epoch 1/10 Iteration: 1500 Avg. Training loss: 3.9131 0.1885 sec/batch\n",
      "Epoch 1/10 Iteration: 1600 Avg. Training loss: 4.0861 0.1972 sec/batch\n",
      "Epoch 1/10 Iteration: 1700 Avg. Training loss: 3.2709 0.1973 sec/batch\n",
      "Epoch 1/10 Iteration: 1800 Avg. Training loss: 4.1077 0.2087 sec/batch\n",
      "Epoch 1/10 Iteration: 1900 Avg. Training loss: 3.9130 0.2030 sec/batch\n",
      "Epoch 1/10 Iteration: 2000 Avg. Training loss: 4.3566 0.2090 sec/batch\n",
      "Nearest to [outlived]: dl, duomo, mug, infinitesimal, destroyer, rodger, formalization, mgb,\n",
      "Nearest to [rated]: psychomotor, obj, worcester, stopwatch, offroad, wavelet, hogshead, whirlpool,\n",
      "Nearest to [dragster]: foonly, functioned, maradns, cnts, clemency, icosahedron, wold, stylists,\n",
      "Nearest to [simplicity]: relocate, presumed, realplayer, government, passenger, winifred, swnt, carpio,\n",
      "Nearest to [amongst]: assur, satirize, longitudinal, pinball, hathaway, halt, bogs, metering,\n",
      "Nearest to [dozenal]: ukrainian, cascades, regrets, sens, pediatrics, anchorite, muralitharan, hawai,\n",
      "Nearest to [enharmonic]: salvadoran, redefinition, creamy, sortie, marching, tacticus, maharishi, demonstrably,\n",
      "Nearest to [ascetical]: roared, desertion, boulez, perceval, colman, cmm, combats, loemker,\n",
      "Nearest to [staffordshire]: lecturer, crossley, barbershop, appeasing, tampico, kyo, dano, pinnacles,\n",
      "Nearest to [authenticity]: chappell, curtiz, mistreated, proctor, choreographer, basilan, congregational, quarterly,\n",
      "Nearest to [treadmill]: unpleasant, lingering, superheroes, elmendorf, disiyyah, abadan, collectivism, presenters,\n",
      "Nearest to [wp]: cutters, webcast, hanssen, ii, erdrich, lazzeri, omits, suitcases,\n",
      "Nearest to [syndicalist]: ocampo, nihilistic, mclaughlin, hairs, kpa, crickets, topographic, earthy,\n",
      "Nearest to [geertz]: miming, thabit, rei, thalassemia, revolutionists, perry, socket, uploaded,\n",
      "Nearest to [caliber]: aoraki, magician, duff, barris, dazzling, macdowell, salome, exciting,\n",
      "Nearest to [hanoverians]: taupin, asimo, plano, akhmad, bitumen, bukit, tuonela, outrun,\n",
      "Epoch 1/10 Iteration: 2100 Avg. Training loss: 4.3588 0.2278 sec/batch\n",
      "Epoch 1/10 Iteration: 2200 Avg. Training loss: 4.5779 0.2030 sec/batch\n",
      "Epoch 1/10 Iteration: 2300 Avg. Training loss: 4.5089 0.2030 sec/batch\n",
      "Epoch 1/10 Iteration: 2400 Avg. Training loss: 4.0736 0.2048 sec/batch\n",
      "Epoch 1/10 Iteration: 2500 Avg. Training loss: 4.4535 0.2057 sec/batch\n",
      "Epoch 1/10 Iteration: 2600 Avg. Training loss: 3.9527 0.2048 sec/batch\n",
      "Epoch 1/10 Iteration: 2700 Avg. Training loss: 4.5222 0.2041 sec/batch\n",
      "Epoch 1/10 Iteration: 2800 Avg. Training loss: 4.4141 0.2033 sec/batch\n",
      "Epoch 1/10 Iteration: 2900 Avg. Training loss: 4.3068 0.2019 sec/batch\n",
      "Epoch 1/10 Iteration: 3000 Avg. Training loss: 4.2875 0.2001 sec/batch\n",
      "Nearest to [outlived]: dl, duomo, mug, infinitesimal, formalization, mgb, zama, letterman,\n",
      "Nearest to [rated]: psychomotor, obj, worcester, stopwatch, wavelet, hogshead, whirlpool, jacks,\n",
      "Nearest to [dragster]: foonly, functioned, maradns, wold, stylists, eclac, outsiders, rhotic,\n",
      "Nearest to [simplicity]: relocate, realplayer, government, immortals, swnt, payton, carpio, winifred,\n",
      "Nearest to [amongst]: assur, longitudinal, metering, sammon, hathaway, pinball, satirize, mozambican,\n",
      "Nearest to [dozenal]: ukrainian, cascades, regrets, sens, pediatrics, anchorite, muralitharan, sep,\n",
      "Nearest to [enharmonic]: salvadoran, redefinition, creamy, sortie, maharishi, demonstrably, marching, conjectural,\n",
      "Nearest to [ascetical]: roared, boulez, perceval, desertion, colman, cmm, combats, loemker,\n",
      "Nearest to [staffordshire]: lecturer, crossley, barbershop, appeasing, tampico, kyo, dano, pinnacles,\n",
      "Nearest to [authenticity]: chappell, curtiz, proctor, inbound, tach, criminally, cassel, congregational,\n",
      "Nearest to [treadmill]: lingering, superheroes, unpleasant, elmendorf, disiyyah, collectivism, abadan, presenters,\n",
      "Nearest to [wp]: cutters, webcast, hanssen, ii, erdrich, omits, suitcases, halfdan,\n",
      "Nearest to [syndicalist]: ocampo, nihilistic, mclaughlin, kpa, crickets, topographic, hairs, keefe,\n",
      "Nearest to [geertz]: miming, thabit, rei, thalassemia, revolutionists, socket, uploaded, diplomatique,\n",
      "Nearest to [caliber]: aoraki, duff, barris, dazzling, macdowell, salome, icosahedron, papandreou,\n",
      "Nearest to [hanoverians]: taupin, asimo, plano, akhmad, bitumen, bukit, tuonela, outrun,\n",
      "Epoch 1/10 Iteration: 3100 Avg. Training loss: 4.5972 0.2053 sec/batch\n",
      "Epoch 1/10 Iteration: 3200 Avg. Training loss: 4.6601 0.2011 sec/batch\n",
      "Epoch 1/10 Iteration: 3300 Avg. Training loss: 4.7570 0.2111 sec/batch\n",
      "Epoch 1/10 Iteration: 3400 Avg. Training loss: 4.6741 0.2022 sec/batch\n",
      "Epoch 1/10 Iteration: 3500 Avg. Training loss: 4.7381 0.2003 sec/batch\n",
      "Epoch 1/10 Iteration: 3600 Avg. Training loss: 4.6485 0.1998 sec/batch\n",
      "Epoch 1/10 Iteration: 3700 Avg. Training loss: 4.7943 0.2024 sec/batch\n",
      "Epoch 1/10 Iteration: 3800 Avg. Training loss: 4.5170 0.2014 sec/batch\n",
      "Epoch 1/10 Iteration: 3900 Avg. Training loss: 4.6032 0.2034 sec/batch\n",
      "Epoch 1/10 Iteration: 4000 Avg. Training loss: 4.8259 0.2008 sec/batch\n",
      "Nearest to [outlived]: dl, duomo, mug, infinitesimal, formalization, mgb, zama, letterman,\n",
      "Nearest to [rated]: psychomotor, obj, stopwatch, wavelet, jacks, hogshead, worcester, whirlpool,\n",
      "Nearest to [dragster]: foonly, functioned, maradns, wold, stylists, eclac, rhotic, icosahedron,\n",
      "Nearest to [simplicity]: payton, acp, relocate, winifred, realplayer, naia, antidiscrimination, government,\n",
      "Nearest to [amongst]: assur, metering, bak, gondar, sammon, uist, bogs, longitudinal,\n",
      "Nearest to [dozenal]: cascades, regrets, sens, pediatrics, muralitharan, anchorite, sep, abiotic,\n",
      "Nearest to [enharmonic]: salvadoran, redefinition, creamy, sortie, demonstrably, conjectural, dems, harakat,\n",
      "Nearest to [ascetical]: roared, boulez, perceval, desertion, colman, cmm, combats, loemker,\n",
      "Nearest to [staffordshire]: crossley, tampico, appeasing, sphincter, barbershop, kyo, smear, depress,\n",
      "Nearest to [authenticity]: curtiz, chappell, proctor, congregational, tach, dyke, criminally, inbound,\n",
      "Nearest to [treadmill]: lingering, superheroes, elmendorf, disiyyah, collectivism, abadan, presenters, regina,\n",
      "Nearest to [wp]: cutters, webcast, hanssen, ii, erdrich, omits, suitcases, secker,\n",
      "Nearest to [syndicalist]: ocampo, nihilistic, kpa, crickets, topographic, hairs, keefe, lcd,\n",
      "Nearest to [geertz]: miming, thabit, rei, thalassemia, revolutionists, socket, uploaded, diplomatique,\n",
      "Nearest to [caliber]: aoraki, duff, barris, macdowell, salome, icosahedron, papandreou, gob,\n",
      "Nearest to [hanoverians]: taupin, asimo, plano, akhmad, bitumen, bukit, tuonela, outrun,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Iteration: 4100 Avg. Training loss: 3.9283 0.2038 sec/batch\n",
      "Epoch 1/10 Iteration: 4200 Avg. Training loss: 4.2016 0.2009 sec/batch\n",
      "Epoch 1/10 Iteration: 4300 Avg. Training loss: 4.6410 0.1998 sec/batch\n",
      "Epoch 1/10 Iteration: 4400 Avg. Training loss: 4.5607 0.1868 sec/batch\n",
      "Epoch 1/10 Iteration: 4500 Avg. Training loss: 4.0827 0.1917 sec/batch\n",
      "Epoch 1/10 Iteration: 4600 Avg. Training loss: 4.4050 0.1901 sec/batch\n",
      "Epoch 1/10 Iteration: 4700 Avg. Training loss: 4.2389 0.1863 sec/batch\n",
      "Epoch 1/10 Iteration: 4800 Avg. Training loss: 4.5417 0.1925 sec/batch\n",
      "Epoch 1/10 Iteration: 4900 Avg. Training loss: 4.7632 0.1876 sec/batch\n",
      "Epoch 1/10 Iteration: 5000 Avg. Training loss: 4.3934 0.1827 sec/batch\n",
      "Nearest to [outlived]: dl, duomo, mug, infinitesimal, mgb, zama, lucio, reuel,\n",
      "Nearest to [rated]: obj, psychomotor, stopwatch, wavelet, whirlpool, jacks, deceptively, hogshead,\n",
      "Nearest to [dragster]: foonly, functioned, maradns, wold, stylists, eclac, icosahedron, cnts,\n",
      "Nearest to [simplicity]: payton, acp, relocate, winifred, naia, antidiscrimination, government, kahtani,\n",
      "Nearest to [amongst]: assur, metering, bak, uist, gondar, sammon, refracted, longitudinal,\n",
      "Nearest to [dozenal]: cascades, regrets, pediatrics, sens, muralitharan, anchorite, sep, abiotic,\n",
      "Nearest to [enharmonic]: salvadoran, redefinition, creamy, sortie, conjectural, dems, harakat, tacticus,\n",
      "Nearest to [ascetical]: roared, boulez, perceval, desertion, colman, cmm, combats, loemker,\n",
      "Nearest to [staffordshire]: crossley, tampico, appeasing, sphincter, barbershop, smear, depress, faroes,\n",
      "Nearest to [authenticity]: curtiz, proctor, congregational, chappell, dyke, inbound, basilan, assorted,\n",
      "Nearest to [treadmill]: lingering, superheroes, elmendorf, disiyyah, collectivism, regina, buddies, didier,\n",
      "Nearest to [wp]: cutters, webcast, hanssen, ii, erdrich, omits, suitcases, secker,\n",
      "Nearest to [syndicalist]: ocampo, nihilistic, crickets, topographic, kpa, keefe, lcd, mclaughlin,\n",
      "Nearest to [geertz]: miming, thabit, rei, thalassemia, socket, uploaded, diplomatique, rectification,\n",
      "Nearest to [caliber]: aoraki, duff, barris, macdowell, icosahedron, papandreou, gob, smear,\n",
      "Nearest to [hanoverians]: taupin, asimo, plano, bitumen, bukit, tuonela, outrun, akhmad,\n",
      "Epoch 1/10 Iteration: 5100 Avg. Training loss: 4.6232 0.1858 sec/batch\n",
      "Epoch 1/10 Iteration: 5200 Avg. Training loss: 3.7158 0.1833 sec/batch\n",
      "Epoch 1/10 Iteration: 5300 Avg. Training loss: 4.4243 0.1826 sec/batch\n",
      "Epoch 1/10 Iteration: 5400 Avg. Training loss: 4.2565 0.1824 sec/batch\n",
      "Epoch 1/10 Iteration: 5500 Avg. Training loss: 4.2552 0.1836 sec/batch\n",
      "Epoch 1/10 Iteration: 5600 Avg. Training loss: 4.0929 0.1834 sec/batch\n",
      "Epoch 1/10 Iteration: 5700 Avg. Training loss: 4.4391 0.2004 sec/batch\n",
      "Epoch 1/10 Iteration: 5800 Avg. Training loss: 4.5450 0.1950 sec/batch\n",
      "Epoch 1/10 Iteration: 5900 Avg. Training loss: 4.3621 0.1829 sec/batch\n",
      "Epoch 1/10 Iteration: 6000 Avg. Training loss: 4.0417 0.1918 sec/batch\n",
      "Nearest to [outlived]: duomo, mug, mgb, zama, dl, infinitesimal, lucio, reuel,\n",
      "Nearest to [rated]: psychomotor, obj, stopwatch, whirlpool, wavelet, hogshead, demarcation, deceptively,\n",
      "Nearest to [dragster]: foonly, maradns, wold, stylists, eclac, icosahedron, cnts, paisiello,\n",
      "Nearest to [simplicity]: payton, acp, naia, kahtani, deceiver, winifred, realplayer, government,\n",
      "Nearest to [amongst]: assur, bak, metering, gondar, bogs, refracted, uist, hathaway,\n",
      "Nearest to [dozenal]: cascades, regrets, pediatrics, sens, sep, abiotic, conurbations, roussimoff,\n",
      "Nearest to [enharmonic]: salvadoran, redefinition, creamy, conjectural, dems, sortie, harakat, tacticus,\n",
      "Nearest to [ascetical]: roared, boulez, perceval, cmm, desertion, colman, lehrer, combats,\n",
      "Nearest to [staffordshire]: crossley, tampico, barbershop, appeasing, sphincter, depress, smear, dano,\n",
      "Nearest to [authenticity]: curtiz, congregational, chappell, dyke, basilan, assorted, proctor, bhojpuri,\n",
      "Nearest to [treadmill]: superheroes, lingering, elmendorf, disiyyah, collectivism, regina, didier, datasheet,\n",
      "Nearest to [wp]: webcast, hanssen, ii, erdrich, cutters, suitcases, secker, camouflaged,\n",
      "Nearest to [syndicalist]: ocampo, nihilistic, crickets, keefe, mclaughlin, germanisation, ceiba, pejoratively,\n",
      "Nearest to [geertz]: miming, thabit, thalassemia, rei, uploaded, diplomatique, rectification, olof,\n",
      "Nearest to [caliber]: aoraki, barris, macdowell, duff, icosahedron, papandreou, gob, smear,\n",
      "Nearest to [hanoverians]: taupin, asimo, plano, bitumen, bukit, tuonela, outrun, akhmad,\n",
      "Epoch 1/10 Iteration: 6100 Avg. Training loss: 3.9500 0.1910 sec/batch\n",
      "Epoch 1/10 Iteration: 6200 Avg. Training loss: 4.1381 0.1908 sec/batch\n",
      "Epoch 1/10 Iteration: 6300 Avg. Training loss: 5.0323 0.1869 sec/batch\n",
      "Epoch 1/10 Iteration: 6400 Avg. Training loss: 3.9378 0.1857 sec/batch\n",
      "Epoch 1/10 Iteration: 6500 Avg. Training loss: 4.8677 0.1939 sec/batch\n",
      "Epoch 1/10 Iteration: 6600 Avg. Training loss: 4.9011 0.1906 sec/batch\n",
      "Epoch 1/10 Iteration: 6700 Avg. Training loss: 4.9197 0.1829 sec/batch\n",
      "Epoch 1/10 Iteration: 6800 Avg. Training loss: 4.3108 0.1913 sec/batch\n",
      "Epoch 1/10 Iteration: 6900 Avg. Training loss: 3.8201 0.1830 sec/batch\n",
      "Epoch 1/10 Iteration: 7000 Avg. Training loss: 4.3397 0.1838 sec/batch\n",
      "Nearest to [outlived]: duomo, mug, dl, mgb, infinitesimal, reuel, turismo, zama,\n",
      "Nearest to [rated]: obj, psychomotor, stopwatch, wavelet, demarcation, hogshead, whirlpool, giuliani,\n",
      "Nearest to [dragster]: foonly, maradns, wold, stylists, eclac, icosahedron, cnts, paisiello,\n",
      "Nearest to [simplicity]: payton, acp, naia, kahtani, realplayer, winifred, promulgates, government,\n",
      "Nearest to [amongst]: assur, bak, gondar, hathaway, mistrust, refracted, metering, longitudinal,\n",
      "Nearest to [dozenal]: cascades, regrets, pediatrics, sens, sep, conurbations, roussimoff, waldorf,\n",
      "Nearest to [enharmonic]: salvadoran, redefinition, creamy, conjectural, dems, sortie, tacticus, biomechanical,\n",
      "Nearest to [ascetical]: roared, boulez, perceval, cmm, desertion, colman, lehrer, combats,\n",
      "Nearest to [staffordshire]: crossley, tampico, appeasing, sphincter, depress, smear, dano, faroes,\n",
      "Nearest to [authenticity]: curtiz, proctor, tach, dyke, chappell, assorted, bhojpuri, bord,\n",
      "Nearest to [treadmill]: lingering, elmendorf, disiyyah, collectivism, didier, datasheet, laberge, jewishencyclopedia,\n",
      "Nearest to [wp]: webcast, hanssen, ii, erdrich, suitcases, secker, wimp, lazzeri,\n",
      "Nearest to [syndicalist]: ocampo, nihilistic, crickets, keefe, mclaughlin, germanisation, ceiba, pejoratively,\n",
      "Nearest to [geertz]: miming, thabit, thalassemia, rei, uploaded, diplomatique, rectification, lekhanya,\n",
      "Nearest to [caliber]: aoraki, barris, duff, icosahedron, papandreou, gob, smear, mta,\n",
      "Nearest to [hanoverians]: taupin, asimo, plano, bitumen, bukit, tuonela, outrun, akhmad,\n",
      "Epoch 1/10 Iteration: 7100 Avg. Training loss: 4.5656 0.1896 sec/batch\n",
      "Epoch 1/10 Iteration: 7200 Avg. Training loss: 3.7660 0.1951 sec/batch\n",
      "Epoch 1/10 Iteration: 7300 Avg. Training loss: 4.2678 0.1888 sec/batch\n",
      "Epoch 1/10 Iteration: 7400 Avg. Training loss: 4.3355 0.1838 sec/batch\n",
      "Epoch 1/10 Iteration: 7500 Avg. Training loss: 4.2626 0.1847 sec/batch\n",
      "Epoch 1/10 Iteration: 7600 Avg. Training loss: 4.6859 0.1830 sec/batch\n",
      "Epoch 1/10 Iteration: 7700 Avg. Training loss: 4.1083 0.1825 sec/batch\n",
      "Epoch 1/10 Iteration: 7800 Avg. Training loss: 4.3318 0.1827 sec/batch\n",
      "Epoch 1/10 Iteration: 7900 Avg. Training loss: 4.0845 0.1824 sec/batch\n",
      "Epoch 1/10 Iteration: 8000 Avg. Training loss: 4.6286 0.1819 sec/batch\n",
      "Nearest to [outlived]: mug, dl, duomo, mgb, reuel, turismo, zama, tuniit,\n",
      "Nearest to [rated]: obj, psychomotor, stopwatch, frown, dragging, wavelet, offroad, paternoster,\n",
      "Nearest to [dragster]: foonly, maradns, wold, stylists, eclac, icosahedron, cnts, paisiello,\n",
      "Nearest to [simplicity]: naia, payton, acp, kahtani, winifred, promulgates, deceiver, sophomores,\n",
      "Nearest to [amongst]: assur, bak, metering, gondar, herbivorous, chinguetti, prevalent, hathaway,\n",
      "Nearest to [dozenal]: cascades, regrets, sens, conurbations, shorthair, pediatrics, travelogues, glycolipids,\n",
      "Nearest to [enharmonic]: salvadoran, redefinition, creamy, conjectural, dems, sortie, biomechanical, palme,\n",
      "Nearest to [ascetical]: roared, boulez, perceval, cmm, desertion, colman, loemker, micronations,\n",
      "Nearest to [staffordshire]: crossley, tampico, appeasing, smear, faroes, dano, gallardo, kyo,\n",
      "Nearest to [authenticity]: curtiz, proctor, tach, dyke, chappell, bhojpuri, bord, criminally,\n",
      "Nearest to [treadmill]: lingering, elmendorf, disiyyah, collectivism, didier, datasheet, laberge, jewishencyclopedia,\n",
      "Nearest to [wp]: webcast, hanssen, ii, erdrich, suitcases, secker, wimp, lazzeri,\n",
      "Nearest to [syndicalist]: ocampo, nihilistic, crickets, keefe, mclaughlin, germanisation, ceiba, pejoratively,\n",
      "Nearest to [geertz]: miming, thabit, thalassemia, rei, uploaded, diplomatique, rectification, lekhanya,\n",
      "Nearest to [caliber]: aoraki, barris, mta, icosahedron, papandreou, laude, smear, duff,\n",
      "Nearest to [hanoverians]: taupin, asimo, plano, bitumen, bukit, tuonela, outrun, akhmad,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Iteration: 8100 Avg. Training loss: 3.7840 0.1942 sec/batch\n",
      "Epoch 1/10 Iteration: 8200 Avg. Training loss: 3.7855 0.1838 sec/batch\n",
      "Epoch 1/10 Iteration: 8300 Avg. Training loss: 3.9699 0.1835 sec/batch\n",
      "Epoch 1/10 Iteration: 8400 Avg. Training loss: 4.5165 0.1829 sec/batch\n",
      "Epoch 1/10 Iteration: 8500 Avg. Training loss: 4.0307 0.1827 sec/batch\n",
      "Epoch 1/10 Iteration: 8600 Avg. Training loss: 3.6334 0.1873 sec/batch\n",
      "Epoch 1/10 Iteration: 8700 Avg. Training loss: 3.8333 0.1914 sec/batch\n",
      "Epoch 1/10 Iteration: 8800 Avg. Training loss: 4.1146 0.2031 sec/batch\n",
      "Epoch 1/10 Iteration: 8900 Avg. Training loss: 4.4231 0.1819 sec/batch\n",
      "Epoch 1/10 Iteration: 9000 Avg. Training loss: 4.1436 0.1822 sec/batch\n",
      "Nearest to [outlived]: mug, dl, duomo, mgb, reuel, turismo, zama, tuniit,\n",
      "Nearest to [rated]: obj, psychomotor, stopwatch, dragging, frown, chondrichthyes, wavelet, paternoster,\n",
      "Nearest to [dragster]: foonly, maradns, wold, stylists, eclac, icosahedron, cnts, paisiello,\n",
      "Nearest to [simplicity]: naia, payton, acp, winifred, sophomores, kahtani, deceiver, promulgates,\n",
      "Nearest to [amongst]: assur, bak, prevalent, herbivorous, metering, refracted, gondar, chinguetti,\n",
      "Nearest to [dozenal]: cascades, regrets, sens, conurbations, shorthair, pediatrics, travelogues, glycolipids,\n",
      "Nearest to [enharmonic]: salvadoran, redefinition, creamy, conjectural, dems, sortie, biomechanical, palme,\n",
      "Nearest to [ascetical]: roared, boulez, perceval, colman, desertion, cmm, loemker, micronations,\n",
      "Nearest to [staffordshire]: crossley, tampico, appeasing, smear, faroes, dano, gallardo, kyo,\n",
      "Nearest to [authenticity]: curtiz, chappell, proctor, bhojpuri, bord, tach, dyke, congregational,\n",
      "Nearest to [treadmill]: collectivism, lingering, disiyyah, didier, datasheet, laberge, elmendorf, jewishencyclopedia,\n",
      "Nearest to [wp]: hanssen, ii, erdrich, suitcases, secker, webcast, wimp, lazzeri,\n",
      "Nearest to [syndicalist]: ocampo, nihilistic, crickets, ceiba, germanisation, mclaughlin, svr, physiol,\n",
      "Nearest to [geertz]: miming, thalassemia, rei, uploaded, diplomatique, rectification, lekhanya, revolutionists,\n",
      "Nearest to [caliber]: aoraki, barris, mta, icosahedron, papandreou, smear, salome, laude,\n",
      "Nearest to [hanoverians]: taupin, asimo, plano, bukit, tuonela, bitumen, outrun, akhmad,\n",
      "Epoch 1/10 Iteration: 9100 Avg. Training loss: 4.3279 0.1888 sec/batch\n",
      "Epoch 1/10 Iteration: 9200 Avg. Training loss: 3.5730 0.1833 sec/batch\n",
      "Epoch 1/10 Iteration: 9300 Avg. Training loss: 4.1759 0.1838 sec/batch\n",
      "Epoch 1/10 Iteration: 9400 Avg. Training loss: 3.9954 0.1861 sec/batch\n",
      "Epoch 1/10 Iteration: 9500 Avg. Training loss: 4.2979 0.1908 sec/batch\n",
      "Epoch 1/10 Iteration: 9600 Avg. Training loss: 4.4975 0.1837 sec/batch\n",
      "Epoch 1/10 Iteration: 9700 Avg. Training loss: 3.9180 0.1838 sec/batch\n",
      "Epoch 1/10 Iteration: 9800 Avg. Training loss: 3.6763 0.1858 sec/batch\n",
      "Epoch 1/10 Iteration: 9900 Avg. Training loss: 3.6846 0.1872 sec/batch\n",
      "Epoch 1/10 Iteration: 10000 Avg. Training loss: 3.0854 0.1873 sec/batch\n",
      "Nearest to [outlived]: dl, duomo, mgb, reuel, turismo, zama, tuniit, frazee,\n",
      "Nearest to [rated]: obj, psychomotor, stopwatch, dragging, frown, chondrichthyes, offroad, paternoster,\n",
      "Nearest to [dragster]: foonly, maradns, wold, stylists, icosahedron, cnts, paisiello, defusal,\n",
      "Nearest to [simplicity]: naia, payton, winifred, sophomores, kahtani, deceiver, promulgates, pathologic,\n",
      "Nearest to [amongst]: assur, prevalent, bak, herbivorous, metering, refracted, gondar, chinguetti,\n",
      "Nearest to [dozenal]: regrets, cascades, sens, conurbations, shorthair, pediatrics, travelogues, glycolipids,\n",
      "Nearest to [enharmonic]: salvadoran, redefinition, creamy, conjectural, dems, sortie, biomechanical, palme,\n",
      "Nearest to [ascetical]: roared, boulez, perceval, cmm, loemker, micronations, transwomen, magnetometer,\n",
      "Nearest to [staffordshire]: crossley, tampico, appeasing, smear, faroes, dano, gallardo, kyo,\n",
      "Nearest to [authenticity]: curtiz, chappell, proctor, congregational, bhojpuri, bord, cyberbrain, tach,\n",
      "Nearest to [treadmill]: collectivism, disiyyah, didier, datasheet, laberge, elmendorf, jewishencyclopedia, lingering,\n",
      "Nearest to [wp]: hanssen, ii, erdrich, suitcases, secker, webcast, wimp, lazzeri,\n",
      "Nearest to [syndicalist]: ocampo, nihilistic, crickets, ceiba, germanisation, mclaughlin, svr, physiol,\n",
      "Nearest to [geertz]: miming, thalassemia, rei, uploaded, diplomatique, rectification, lekhanya, revolutionists,\n",
      "Nearest to [caliber]: aoraki, barris, icosahedron, papandreou, smear, salome, laude, duff,\n",
      "Nearest to [hanoverians]: taupin, asimo, tuonela, bitumen, outrun, akhmad, bukit, biham,\n",
      "Epoch 1/10 Iteration: 10100 Avg. Training loss: 3.9603 0.1954 sec/batch\n",
      "Epoch 1/10 Iteration: 10200 Avg. Training loss: 3.6368 0.1887 sec/batch\n",
      "Epoch 1/10 Iteration: 10300 Avg. Training loss: 3.8160 0.2053 sec/batch\n",
      "Epoch 1/10 Iteration: 10400 Avg. Training loss: 4.1614 0.1892 sec/batch\n",
      "Epoch 1/10 Iteration: 10500 Avg. Training loss: 4.3596 0.1871 sec/batch\n",
      "Epoch 1/10 Iteration: 10600 Avg. Training loss: 3.7380 0.2010 sec/batch\n",
      "Epoch 1/10 Iteration: 10700 Avg. Training loss: 4.1232 0.2111 sec/batch\n",
      "Epoch 1/10 Iteration: 10800 Avg. Training loss: 3.7871 0.1924 sec/batch\n",
      "Epoch 1/10 Iteration: 10900 Avg. Training loss: 3.5300 0.1924 sec/batch\n",
      "Epoch 1/10 Iteration: 11000 Avg. Training loss: 3.8851 0.1939 sec/batch\n",
      "Nearest to [outlived]: duomo, mgb, reuel, turismo, zama, tuniit, frazee, infinitesimal,\n",
      "Nearest to [rated]: obj, psychomotor, stopwatch, dragging, frown, chondrichthyes, witwatersrand, offroad,\n",
      "Nearest to [dragster]: foonly, maradns, wold, stylists, icosahedron, cnts, paisiello, defusal,\n",
      "Nearest to [simplicity]: naia, payton, winifred, sophomores, kahtani, deceiver, promulgates, pathologic,\n",
      "Nearest to [amongst]: prevalent, assur, spartan, bak, herbivorous, chinguetti, plenty, hydrofoil,\n",
      "Nearest to [dozenal]: regrets, cascades, sens, conurbations, shorthair, pediatrics, travelogues, muralitharan,\n",
      "Nearest to [enharmonic]: salvadoran, redefinition, creamy, conjectural, dems, sortie, biomechanical, palme,\n",
      "Nearest to [ascetical]: roared, boulez, perceval, cmm, loemker, micronations, transwomen, magnetometer,\n",
      "Nearest to [staffordshire]: tampico, appeasing, crossley, smear, faroes, dano, gallardo, kyo,\n",
      "Nearest to [authenticity]: curtiz, chappell, congregational, bhojpuri, bord, cyberbrain, tach, criminally,\n",
      "Nearest to [treadmill]: collectivism, disiyyah, didier, datasheet, laberge, elmendorf, jewishencyclopedia, larp,\n",
      "Nearest to [wp]: hanssen, ii, erdrich, suitcases, secker, wimp, cutters, lazzeri,\n",
      "Nearest to [syndicalist]: ocampo, nihilistic, crickets, ceiba, germanisation, mclaughlin, svr, physiol,\n",
      "Nearest to [geertz]: thalassemia, miming, rei, uploaded, diplomatique, rectification, lekhanya, revolutionists,\n",
      "Nearest to [caliber]: aoraki, barris, papandreou, salome, jockeys, smear, icosahedron, duff,\n",
      "Nearest to [hanoverians]: taupin, asimo, tuonela, bitumen, outrun, akhmad, bukit, biham,\n",
      "Epoch 1/10 Iteration: 11100 Avg. Training loss: 3.6117 0.1899 sec/batch\n",
      "Epoch 1/10 Iteration: 11200 Avg. Training loss: 4.0018 0.2011 sec/batch\n",
      "Epoch 1/10 Iteration: 11300 Avg. Training loss: 4.1423 0.2031 sec/batch\n",
      "Epoch 1/10 Iteration: 11400 Avg. Training loss: 3.8695 0.2052 sec/batch\n",
      "Epoch 1/10 Iteration: 11500 Avg. Training loss: 4.1136 0.1910 sec/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-ee1415117f5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mfeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 887\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    888\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1110\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1286\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1287\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1290\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1292\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1293\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1277\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10 # 迭代次数\n",
    "batch_size = 100 # batch大小\n",
    "window_size = 10 # 窗口大小\n",
    "\n",
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver() # 文件存储\n",
    "    \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for e in range(1, epochs+1):\n",
    "        batches = get_batches(train_words, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        \n",
    "        for x,y in batches:\n",
    "            feed = {inputs:x, labels:np.array(y)[:,None]}\n",
    "            train_loss, _ = sess.run([cost,optimizer], feed_dict = feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100 == 0:\n",
    "                end = time.time()\n",
    "                print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "                \n",
    "            # 计算相似的词\n",
    "            if iteration % 1000 == 0:\n",
    "                # 计算similarity\n",
    "                sim = similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_word = int_to_vocab[valid_examples[i]]\n",
    "                    top_k = 8 # 取最相似的单词的前8个\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                    log = 'Nearest to [%s]:' % valid_word\n",
    "                    for k in range(top_k):\n",
    "                        close_word = int_to_vocab[nearest[k]]\n",
    "                        log = '%s %s,' % (log, close_word)\n",
    "                    print(log)\n",
    "            \n",
    "            iteration += 1\n",
    "            \n",
    "        save_path = saver.save(sess, \"checkpoints/text8.ckpt\")\n",
    "        embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embed_mat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-16d6594571c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mviz_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtsne\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0membed_tsne\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed_mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mviz_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'embed_mat' is not defined"
     ]
    }
   ],
   "source": [
    "viz_words = 500\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :]), color='steelblue'\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
