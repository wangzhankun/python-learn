{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepocess(text, freq=5):\n",
    "    \"\"\"输入文本并对文本进行预处理。处理包括：将文本句子转为单个的按原有文本顺序排列的列表，\n",
    "    同时去除频数低于5的噪声，此外对高频词汇进行subsampling处理\n",
    "    \n",
    "    Arguments:\n",
    "        text {str} -- 输入文本\n",
    "        freq {int} -- 频数上线\n",
    "    \n",
    "    Returns:\n",
    "        list -- 训练集\n",
    "    \"\"\"\n",
    "    # 输入为文本,str类型\n",
    "    # 输出为处理后的单词列表\n",
    "    # 同时进行subsampling\n",
    "    words = [word for word in text.split()]\n",
    "    # 使用nltk便于处理\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    words_count = Counter(words)\n",
    "    #删除单词数目少于5的噪声\n",
    "    words = [tokenizer.tokenize(word.lower())[0]  for word in words if words_count[word] > freq]\n",
    "    \n",
    "    # subsampling\n",
    "    threshold = 1e-5\n",
    "    total_words = len(words)\n",
    "    freqs = {word:count/total_words for word, count in words_count.items()}\n",
    "    p_drop = {word:1-np.sqrt(threshold/freqs[word]) for word in words_count}\n",
    "    train_words = [word for word in words if random.random() < (1-p_drop[word])]\n",
    "    \n",
    "    return train_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(words, idx, window_size=5):\n",
    "    '''\n",
    "    获得input word的上下文单词列表\n",
    "    words: 单词列表\n",
    "    idx: input words的索引号\n",
    "    window_size: 窗口大小\n",
    "    '''\n",
    "    target_window = np.random.randint(1, window_size+1) # 随机生成目标窗口大小\n",
    "    # 这里要考虑input word前面单词不够的情况\n",
    "    start_point = idx - target_window if (idx - target_window) > 0 else 0\n",
    "    end_point = idx + target_window\n",
    "    targets = set(words[start_point:idx] + words[idx+1:end_point+1])\n",
    "    return list(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size = 5):\n",
    "    '''\n",
    "    构造一个获取batch的生成器\n",
    "    '''\n",
    "    # 有多少个小批量，每个批量的单词数是batch_size\n",
    "    n_batches = len(words) // batch_size\n",
    "    \n",
    "    # 仅取full batches\n",
    "    words = words[:n_batches * batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x,y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for i in range(len(batch)):\n",
    "            # input word\n",
    "            batch_x = batch[i]\n",
    "            # output word\n",
    "            batch_y = get_targets(batch, i, window_size)\n",
    "            # 由于一个input word会对应多个output word，因此需要统一长度\n",
    "            x.extend([batch_x] * len(batch_y))\n",
    "            y.extend(batch_y)\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行图定义\n",
    "def define_graph(train_graph):\n",
    "    with train_graph.as_default():\n",
    "        # 输入，定义placeholder\n",
    "        with tf.name_scope(\"inputs\"):\n",
    "            inputs = tf.placeholder(tf.int32, shape=[None], name='inputs')\n",
    "            labels = tf.placeholder(tf.int32, shape=[None, None], name='labels')\n",
    "\n",
    "        # 嵌入层权重矩阵\n",
    "        embedding = tf.Variable(tf.random_uniform([embedding_row_dim, embedding_col_dim]), name='embedding')\n",
    "        # 实现lookup\n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "\n",
    "        # 运算层\n",
    "        # 注意name_scope中不能有“：”，否则会报错\n",
    "        with tf.name_scope('layer_with_learn_skip_' + str(learn_skip)):\n",
    "            # 权重矩阵\n",
    "            with tf.name_scope(\"weights\"):\n",
    "                Weight = tf.Variable(tf.truncated_normal([embedding_row_dim, embedding_col_dim], stddev = 0.1), name='Weight')\n",
    "                tf.summary.histogram('embedding/weights', Weight)\n",
    "            # 偏差。官方推荐不初始化为0    \n",
    "            with tf.name_scope('biases'):\n",
    "                biase = tf.Variable(tf.zeros(embedding_row_dim)+0.1, name='biase')\n",
    "                tf.summary.histogram('embedding/biases', biase)\n",
    "            # 定义损失函数    \n",
    "            with tf.name_scope('loss'):\n",
    "                loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(Weight, biase, labels, embed, num_sampled, vocab_size))\n",
    "                tf.summary.scalar('loss', loss)\n",
    "            # 优化器\n",
    "            with tf.name_scope('train'):\n",
    "                optimizer = tf.train.AdamOptimizer(learn_skip).minimize(loss)\n",
    "                \n",
    "        # 保存图\n",
    "        saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "def train(train_graph, dataset, batch_size=100, window_size=5, save_graph_path=\"final_skip_gram_model\"):\n",
    "    \n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(epoches):\n",
    "            batches = get_batches(dataset, batch_size, window_size) #迭代器\n",
    "            for x,y in batches:#小批量随机梯度下降\n",
    "                feed = {inputs:x, labels:np.array(y)[:None]}\n",
    "                sess.run([loss,optimizer], feed_dict = feed)\n",
    "                \n",
    "            # 每个周期都进行一次保存操作\n",
    "            save_path = saver.save(sess, save_graph_path+'/skip_gram'+str(epoch)+'.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"E:/aboutme/STUDY/python-learn/word2vec_skipgram/text8\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "#print(len(text))\n",
    "#print(text[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = prepocess(text) # 得到的最终的训练集\n",
    "#print(\"train words:\", words[:5])\n",
    "\n",
    "# 构建映射表\n",
    "vocabulary = set(words) # 独特词汇表\n",
    "vocab_int = {word:num for num,word in enumerate(vocabulary)}\n",
    "int_vocab = {num:word for num,word in enumerate(vocabulary)}\n",
    "\n",
    "\n",
    "# embedding 的大小\n",
    "vocab_size = len(vocabulary)\n",
    "embedding_row_dim = vocab_size\n",
    "embedding_col_dim = 200\n",
    "\n",
    "save_graph_path = \"final_skip_gram_model\"\n",
    "\n",
    "learn_skip = 0.001 # 学习率\n",
    "epoches = 100 # 训练周期\n",
    "\n",
    "num_sampled = 5 # 负例数量\n",
    "window_size = 5 # 上下文数目\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "define_graph(train_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"inputs/inputs:0\", shape=(?,), dtype=int32) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1049\u001b[0m             subfeed_t = self.graph.as_graph_element(\n\u001b[1;32m-> 1050\u001b[1;33m                 subfeed, allow_tensor=True, allow_operation=False)\n\u001b[0m\u001b[0;32m   1051\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3487\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3488\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3566\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3567\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3568\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor Tensor(\"inputs/inputs:0\", shape=(?,), dtype=int32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-cab7a8b4c926>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-6345369c03f9>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_graph, dataset, batch_size, window_size, save_graph_path)\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#小批量随机梯度下降\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                 \u001b[0mfeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                 \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;31m# 每个周期都进行一次保存操作\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 887\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    888\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1051\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m             raise TypeError(\n\u001b[1;32m-> 1053\u001b[1;33m                 'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\n\u001b[0m\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"inputs/inputs:0\", shape=(?,), dtype=int32) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "train(train_graph, dataset=words[:100*10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
