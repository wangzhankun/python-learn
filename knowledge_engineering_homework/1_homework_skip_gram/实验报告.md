# Skip-Gram 的python实现

词向量表示是NLP各项任务完成的基础。几十年来，研究者对词向量表示进行了深入的研究。2013年，Mikolov等人同时提出了CBOW和Skip-Gram模型，希望高效获取词向量。同年，又提出了skip-gram的改进模型，引入了Hierarchical Softmax和Negative Sampling。

完成工作：

* 代码实现了skip-gram
* 对模型训练结果进行了验证
* 小批量数据集（507k）验证

在这个实验中，我实现了skip gram的模型并进行了训练研究。整个实验在云平台运行30个周期耗时五小时。云平台基本参数为：

* CPU I7 7700K
* GPU 1080Ti
* 数据集：text8
* 框架：TensorFlow+python3.6

## Skip-Gram模型简介

注明：这里的内容编排主要参考了如下内容：

* https://zhuanlan.zhihu.com/p/29527402
* https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html
* 《深度学习原理与实践》陈仲铭  人民邮电出版社



![1561302228994](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1561302228994.png)

在Word2Vec模型中，CBOW是给定上下文，预测input word， skip-gram则是给定中心词预测上下文。

Skip-Gram的训练模型分成两部分，一部分是建立模型，另一部分是通过模型获取词向量表示。首先是要搭建一个神经网络，然后将数据喂入模型并进行训练。训练结束之后，我们即可得到word vector。

### 1. 基本过程

1. 选取中心词以及上下文。

   在实际训练过程中我们可以按照顺序选择中心词，同时选择window-size数量的上下文。通过中心词和2*window-size大小的上下文构成inputs作为数据喂入神经网络。数据格式我定义为两个list，第一个作为x即是中心词，第二个作为y即是上下文。

   例如：对于“我爱北京天安门”。当选择window-size=2时，当“京”是中心词时，我们可以得到输入：

   x=['京'，‘京’，‘京’，‘京’]，y=[‘爱’，‘北’，‘天’，‘安’]

2. 神经网络会根据我们的输入得到一个概率分布，这个概率代表着我们字典中的每个词是output word的可能性。

3. 之后计算损失，按照BP算法进行Weights、embedding、biase的调整

### 2. 神经网络

![1561303875835](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1561303875835.png)



我们整个的神经网络并不复杂。只有一层，之后并且没有非线性函数处理，直接连接softmax层进行输出即可。

下面分章节对基本内容讲解。

#### 单词编码

##### one-hot

one-hot就是利用$R^{|V| \times 1}$向量表示单词。其中$|V|$是单词数量。

我们用$w_c$表示目标单词的one-hot向量。


##### word embedding

例如：
$$
v_c = \left[
        \begin{matrix}
        0.2 \\
        0.5  \\
        ...  \\
        0.1
        \end{matrix}
        \right]
$$
这是一个词向量表示。其中我们用$v_c$目表示单词的词向量。
#### 词向量矩阵

词向量矩阵就是所有单词的词向量集合。这也是我们希望得到的

另外一个矩阵由除了目标单词外的其他单词的**词向量的转置**组成的矩阵，用$W \in R^{V \times d}$。

#### 单词相似度

两个单词求内积。


#### Softmax

Softmax函数的本质是将一个K维的任意实数向量，压缩（映射）成另一个K维的实数向量，其中向量中的每个元素取指都介于（0,1）范围内。

Softmax是对逻辑回归（Logistic Regression, LR）的推广，逻辑回归用于处理二分类问题，其推广Softmax回归则用于处理多分类问题。如下图示，在数学上，Softmax函数会返回输出类的互斥概率分布，例如，网络的输出为（1,1,1,1），经过Softmax函数后输出为（0.25,0.25,0.25,0.25）。我们可以得到分类中唯一所属类别，因此通常把Softmax作为输出层的激活函数。
$$
softmax(x) = \frac{e^{x_j}}{\sum_{k=1}^{K}e^{x_k}}
\\
softmax(x) = softmax(x+c),其中c为常数
$$

下面进一步举例说明。假设有一个多分类问题，但是我们只关心这些类别的最高得分的概率，那么会使用一个带有最大似然估计函数的Softmax输出层来获得所有类比输出概率的最大值。例如神经网络的分类有3个，分别为“野马”“河马”“斑马”，使用softmax作为输出层的激活函数最后只能得到一个最大的分类概率如野马（0.6），河马（0.1），斑马（0.3），其中最大值野马（0.6）。

如果要求每次的输出都可以获得多个分类，例如希望神经网络的预测输出既像“河马”也像“野马”，那么我们不希望Softmax作为输出层，这里可以使用Sigmoid函数作为输出层的激活函数更合适，因为Sigmoid函数可以为每个类别的输出提供独立的概率。


$$
softmax(x) = softmax(x+c),其中c为实数
$$
我们需要知道的是softmax函数就是能够把输入转换为概率分布，也就是说使输入的实数变成分数。
$$
\sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^{K}e^{z_k}}
\\
z = u^T_xv_c
$$

```python
import numpy as np
def softmax(x):
    """
    对输入x的每一行计算softmax
    参数：
    x: 矩阵
    返回值：
    x: 对应矩阵
    """
    origin_shape = x.shape
    
    if len(x.shape) > 1:
        # 矩阵
        tmp = np.max(x, axis=1)
        x -= tmp.reshape((x.shape[0], 1))
        x = np.exp(x)
        tmp = np.sum(x, axis = 1) # 每行求和
        x /= tmp.reshape((x.shape[0]), 1)
        
    else:
        tmp = np.max(x)
        x -= tmp
        x = np.exp(x)
        tmp = np.sum(x)
        x /= tmp
    
    return x
```



#### 理解算法过程

求相似度$u^T_xv_c$步骤：
1. 求$v_c$
   $$
   v_c = Ww_c
   \\
    W \in R^{d \times V}
    \\
    w_c \in R^{V \times 1}
    v_c \in R^{d \times 1}
   $$
   求$v_c$就是在$W$中进行索引的过程，在实际操作中，采用索引进行。
   
   ![1561305017231](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1561305017231.png)
   
   如图所示，实际上直接取矩阵第四行即可。
   
2. 求$u^T_xv_c$
   
    相似度可有矩阵$W v_c$求得。$u^T_xv_c$就是结果的每一行向量
    
3. 求softmax
   
    这步比较简单，把得到的相似度矩阵代入softmax公式，就得到了一个满足概率分布的矩阵。

至此，我们实现了我们的目标：得到一个向量。
$$
W v_c

=

\left[
\begin{matrix}
u_1^Tv_c
\\
u_2^Tv_c
\\
u_3^Tv_c
\\
...
\\
u_V^Tv_c
\\
\end{matrix}
\right]

\stackrel{softmax}{\longrightarrow}

\left[
\begin{matrix}
P(u_1 | v_c)
\\
P(u_2 | v_c)
\\
P(u_3 | v_c)
\\
...
\\
P(u_V | v_c)
\\
\end{matrix}
\right]
$$

4.  之后根据选取的损失函数进行梯度下降训练即可

   

### 3. 负采样

在上面分析中，我们得到了随机梯度下降训练embedding的方法。但是问题是每一次的对参数的修正都是对整个参数的修改，而大量都是无用的修改，造成了大量的计算浪费，这是不可接受的。

另外一方面，我们在softmax函数中，需要对所有单词进行计算得分并求和，计算量也是难以接受。

由此，提出了负采样的概念。

负采样的核心就是：计算目标单词和窗口中的单词的真实单词对“得分”，再加一些“噪声”，即词表中的随机单词和目标单词的“得分”。

### 4. 高频抽样

在实际文本中，一些虚词如“the”占据很高的频率，而这些词本身不会对模型训练产生优化甚至会降低模型准确率，因此我们需要对高频词汇进行降频。

选取的公式为
$$
P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}}
$$

## 代码实现

这里我将代码分成了两部分。一部分是数据预处理数据过程。这一步主要是为了保证在每一次训练中使用的数据都是同一套，避免因为数据处理过程中的随机导致数据不一致。另一部分就是神经网络的实现与训练。

### 数据预处理

在这一部分首先要对数据进行清洗。为方便起见，采用了nltk工具对文本进行标点清理和分词。

```python
#%%
import tensorflow as tf
from nltk.tokenize import WordPunctTokenizer
import numpy as np
from collections import Counter
import random
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import time
#%%
def prepocess(text, freq=5):
    """输入文本并对文本进行预处理。处理包括：将文本句子转为单个的按原有文本顺序排列的列表，
    同时去除频数低于5的噪声，此外对高频词汇进行subsampling处理
    
    Arguments:
        text {str} -- 输入文本
        freq {int} -- 频数上线
    
    Returns:
        list -- 训练集
    """
    # 使用nltk便于处理
    tokenizer = WordPunctTokenizer()
    words = tokenizer.tokenize(text)
    words_count = Counter(words)  # 每个单词的对应的数量
    words = [word.lower() for word in words if words_count[word] > freq]
	
    # 高频抽样代码参考自网络
    # subsampling
    t = 1e-5
    threshold = 0.8 # 剔除概率阈值
    # 统计单词出现频次,这里必须重新计算
    words_count = Counter(words) # 每个单词的对应的数量
    total_count = len(words) # 所有单词的总数
    # 计算频次
    word_freqs = {word:count/total_count for word,count in words_count.items()}
    # 计算被删除的概率
    delete_probability = {word: 1-np.sqrt(t / word_freqs[word]) for word in words_count}
    # 对单词进行采样
    train_words  = [word for word in words if delete_probability[word] < threshold]
    
    return train_words

```

### 神经网络搭建

为了便于查看审阅，我使用了tensorboard进行可视化。下图示神经网络的图示化结果。

![](G:\Download\graph_large_attrs_key=_too_large_attrs&limit_attr_size=1024&run=.png)

![](G:\Download\graph_large_attrs_key=_too_large_attrs&limit_attr_size=1024&run= (1).png)

```python
train_graph = tf.Graph()

with train_graph.as_default():
    # 输入，定义placeholder
    with tf.name_scope("inputs"):
        inputs = tf.placeholder(tf.int32, shape=[None], name='inputs')
        labels = tf.placeholder(tf.int32, shape=[None, None], name='labels')

    # 嵌入层权重矩阵
    with tf.name_scope('embedding'):
        embedding = tf.Variable(tf.random_uniform([embedding_row_dim, embedding_col_dim]), name='embedding')
        tf.summary.histogram('embedding/embedding', embedding)
    # 实现lookup
    embed = tf.nn.embedding_lookup(embedding, inputs)

    # 运算层
    # 注意name_scope中不能有“：”，否则会报错
    with tf.name_scope('layer_with_learn_skip_' + str(learn_skip) +"_num_sampled" + str(num_sampled) + "_dim_" + str(embedding_col_dim)):
        # 权重矩阵
        with tf.name_scope("weights"):
            Weight = tf.Variable(tf.truncated_normal([embedding_row_dim, embedding_col_dim], stddev = 0.1), name='Weight')
            tf.summary.histogram('embedding/weights', Weight)
           
        with tf.name_scope('biases'):
            biase = tf.Variable(tf.zeros(embedding_row_dim)+0.1, name='biase')
            tf.summary.histogram('embedding/biases', biase)
        # 定义损失函数    
        with tf.name_scope('loss'):
            loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(Weight, biase, labels, embed, num_sampled, vocab_size))
            tf.summary.scalar('loss', loss)
        # 优化器
        with tf.name_scope('train'):
            optimizer = tf.train.AdamOptimizer().minimize(loss)
```

### 神经网络训练

```python
if debug == True:
    train(train_graph=train_graph, dataset=dataset[:100*10], batch_size=100, window_size=5, epoches=100,save_path=save_path)
else:
    train(train_graph=train_graph, dataset=dataset, batch_size=100, window_size=5, epoches=30,save_path=save_path)
```



```python
def train(train_graph, dataset, batch_size=100, window_size=5, epoches=100, save_path="final_skip_gram_model"):

    with tf.Session(graph=train_graph) as sess:
        merged = tf.summary.merge_all()
        writer = tf.summary.FileWriter(save_path+"logs/",sess.graph)
        sess.run(tf.global_variables_initializer())
        # 保存图
        saver = tf.train.Saver(max_to_keep=3)
        

        with open(save_path+"cost.txt", "w") as f:
            for epoch in range(epoches):
                batches = get_batches(dataset, batch_size, window_size) #迭代器
                cost = 0
                
                for x,y in batches:#小批量随机梯度下降
                    feed = {inputs:x, labels:np.array(y)[:,None]}
                    train_cost,_ = sess.run([loss,optimizer], feed_dict = feed)
                    cost += train_cost
                    
                f.write(str(cost) + " " + str(epoch+1) + " ")
                    
                # 每个周期都进行一次保存操作
                if debug == False:
                    x,y = get_valid_feed(dataset)
                    feed = {inputs:x, labels:np.array(y)[:,None]}
                    result = sess.run(merged, feed_dict = feed)
                    writer.add_summary(result, epoch)
                else:
                    x,y = get_valid_feed(dataset)
                    feed = {inputs:x, labels:np.array(y)[:,None]}
                    result = sess.run(merged, feed_dict=feed)
                    writer.add_summary(result, epoch)
                    
                saver.save(sess, save_path + 'model/skip_gram'+str(epoch)+'.ckpt')
```

## 结果验证

如下图，是随机选择的点图。从下面的结果可以看出实验是达到了一定的效果的。最接近词计算部分为方便查看进行了翻译。

![](G:\Download\Final_Commit\skip_gram.png)

```

Nearest to nejd: hejaz,  zakat,  wahhabis,  saud,  wahhabi,  imams,  emirs,  emir, 
Nearest to riotous: brant,  crematoria,  dido,  qimei,  encroachments,  manipulators,  thwarting,  erebus, 
Nearest to waltz: woogie,  tude,  fidelio,  webern,  liszt,  debussy,  brahms,  retrograde, 
Nearest to jennifer: elena,  carvalho,  shaffer,  teri,  xavier,  protege,  teenager,  poto, 
Nearest to longitudinally: ediacara,  toner,  amigabasic,  preamplifier,  crescentic,  swnts,  hypnotizable,  cartons, 
Nearest to anglicanism: lutheranism,  puritan,  pentecostal,  episcopacy,  presbyterian,  presbyterianism,  calvinism,  anabaptism, 
Nearest to kroq: unchained,  fundraiser,  clowning,  dropkick,  sideman,  serts,  spacewalk,  wc, 
Nearest to praxis: philosophie,  theoria,  wissenschaften,  phenomenological,  logik,  reassessment,  ologie,  vertebrates, 
Nearest to assab: nanpa,  pfdj,  rastislav,  okresy,  lobito,  uat,  hailstone,  taifa, 
Nearest to captivated: bezprym,  koenigsegg,  graphe,  succesor,  erlanger,  baladan,  adret,  misrepresentation, 
Nearest to tunable: valved,  sputtering,  mosfets,  arsenide,  zildjian,  attenuation,  bandgap,  micelle, 
Nearest to derose: dtds,  cycl,  monists,  sieves,  olympiade,  tansley,  victimization,  kornfeld, 
Nearest to llewelyn: boney,  johansen,  hazel,  pehr,  hellmuth,  franken,  oddie,  hatchback, 
Nearest to izanami: kura,  amaterasu,  goddess,  kami,  bhumi,  churned,  jellies,  athene, 
Nearest to deforestation: desertification,  droughts,  erosion,  nyos,  overgrazing,  ecosystems,  overfishing,  upland, 
Nearest to lowball: jackpots,  joker,  gardena,  deuce,  bettor,  betting,  jackpot,  blackjack,
```

```
最近的nejd：hejaz，zakat，wahhabis，saud，wahhabi，imams，emirs，emir，
最接近骚乱：布兰特，火葬场，迪奥，奇美，侵占，操纵者，挫败，埃里伯斯，
华尔兹最近：woogie，tude，fidelio，webern，liszt，debussy，brahms，retrograde，
jennifer附近的：elena，carvalho，shaffer，teri，xavier，protege，teen，poto，
最接近纵向：ediacara，碳粉，amigabasic，前置放大器，新月形，屁股，催眠，纸箱，
最接近英国国教的：路德教，清教徒，五旬节派，主教，长老会，长老会，加尔文主义，再洗礼，
最近的kroq：unchained，筹款活动，小丑，dropkick，sideman，serts，spacewalk，wc，
最接近实践：哲学，理论，智慧，现象学，逻辑学，重新评估，ologie，脊椎动物，
最近的assab：nanpa，pfdj，rastislav，okresy，lobito，uat，hailstone，taifa，
最近被迷住了：bezprym，koenigsegg，graphe，succesor，erlanger，baladan，adret，misrepresentation，
最接近可调谐：阀门，溅射，mosfets，砷化物，zildjian，衰减，带隙，胶束，
最近的贬低：dtds，周期，一元论，筛子，奥林匹克，坦斯利，受害者，kornfeld，
llewelyn最近的：boney，johansen，hazel，pehr，hellmuth，franken，oddie，掀背车，
最接近izanami：kura，amaterasu，女神，kami，bhumi，搅拌，果冻，雅典娜，
最近砍伐森林：荒漠化，干旱，侵蚀，nyos，过度放牧，生态系统，过度捕捞，高地，
低球最近：jackpots，joker，gardena，deuce，bettor，投注，大奖，二十一点，
```

另外通过将我训练的embedding作为第二个实验的输入也从侧面证明了模型的正确性。从下表可以看出，当训练周期增加20次后，实验效果得到了明显改善。说明训练是有效的，实验结果是可信的。

![1561564996469](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1561564996469.png)

<center>引用自同组同学实验二数据</center>



## 分析

### 结果分析

为了方便分析，我取了508k的文本进行了训练。显示loss的变化曲线如图一。在初期loss出现大幅度波动怀疑是在初期对某些词汇具有较好的相容性。通过loss曲线可以看出随着训练周期的增加loss会逐渐减小，所以总体而言，模型训练是有效的。

由于text8数据量过于庞大，仅运行30次就耗费了五小时，所以没有进行深层次训练。图二是每次选取了1000个中心词作为feed计算出来的结果。从理论上讲，与小批量文本作为训练的结果进行比较现在的embedding仍然不够好，但是由于资金和时间所限，仅在云GPU训练了5个小时。从小批量实验数据100个周期的训练效果来，如果大规模数据训练周期加多会使得embedding的效果更好。

在实验过程中选取了0.01作为学习率。

```python
x,y = get_valid_feed(dataset,valid_size=len(dataset))
                    feed = {inputs:x, labels:np.array(y)[:,None]}
                    result = sess.run(merged, feed_dict=feed)
                    writer.add_summary(result, epoch)
```

![1561561648207](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1561561648207.png)

<center>图一，小批量100周期的loss</center>

![1561565397516](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1561565397516.png)

<center>图二，小批量100训练周期的embedding历史</center>



![1561563092905](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1561563092905.png)

<center>图三，大规模30周期loss，选取1000个中心词</center>

![1561565313147](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1561565313147.png)



<center>图四，大规模30训练周期embedding的历史变化</center>

