
# coding: utf-8

# # 中英文分词方法及实现

# ---

# ### 实验介绍

# 分词作为自然语言处理实际运用当中当中最基本的任务，对文本预处理来说十分的重要。本次实验，我们将对中英文分词做简单介绍，并用 Python 实现对英文分词以及中文分词的正向最大匹配算法。

# ### 实验知识点

# - 英文分词
# - 中文分词简介
# - 中文分词算法
# - Python 实现

# ### 目录索引

# - <a href="#英文分词">英文分词</a>
# - <a href="#中文分词">中文分词</a>
# - <a href="#正向最大匹配法算法实现">正向最大匹配法算法实现</a>

# ---

# 在语言理解中，词是最小的能够独立活动的有意义的粒度。由词到句，由句成文。因此，将词确定下来是理解自然语言处理的第一步，只有跨越了这一步，才能进行后续任务。下面我们通过两个例子来看看中英文分词。

# - 英文原文： `i have a pen,i have an apple!`
# - 英文分词结果： `i` , `have` , `a` , `pen` , `i` , `have` , `an` , `apple` , `!`

# - 中文原文： 我们今天要做实验。
# - 中文分词结果：我们 / 今天 / 要 / 做/ 实验  / 。

# ### 英文分词

# 通过上面的英文分词例子，可以发现英文文本词与词之间有空格或者标点符号，如果想要对这种普通的英文文本进行分词的话是不需要什么算法支撑，暴力拆分，即直接通过空格或者标点来将文本进行分开就可以完成英文分词。

# 这里主要用到实验一介绍的 `.split()` 方法，下面我们来回顾一下 `.split()` 的用法。

# 按空格切分无限次：

# In[ ]:


a = 'i have a pen'
a1 = a.split(' ')
a1


# 按空格切分 1 次：

# In[ ]:


a2 = a.split(' ', 1)
a2


# 按 have 切分 1 次：

# In[ ]:


a3 = a.split('have', 1)
a3


# 我们下面来实现对多个英文文本分词，要求同时以 `,` ， `.` ， `?` ， `!` ， ` ` 五个符号分词。

# 为了方便调用，我们将代码写函数当中。首先对原文本以其中 `1` 个规则切分后，再对分好后的文本进行下一个规则的切分，再对分好的文本进行切分，直到按5个规则切分完成，最后将切分好的词添加进 `tokenized_text` 并返回。代码乍看很长，实际却十分简单。

# In[ ]:


def tokenize_english_text(text):

    # 首先，我们按照标点来分句

    # 先建立一个空集用来，用来将分好的词添加到里面作为函数的返回值
    tokenized_text = []

    # 一个text中可能不止一个内容，我们对每个文本单独处理并存放在各自的分词结果中。
    for data in text:

        # 建立一个空集来存储每一个文本自己的分词结果，每对data一次操作我们都归零这个集合
        tokenized_data = []

        # 以 '.'分割整个句子，对分割后的每一小快s：
        for s in data.split('.'):

            # 将's'以 '？'分割，分割后的每一小快s2：
            for s1 in s.split('?'):

                # 同样的道理分割s2，
                for s2 in s1.split('!'):

                    # 同理
                    for s3 in s2.split(','):

                        # 将s3以空格分割，然后将结果添加到tokenized_data当中
                        tokenized_data.extend(
                            s4 for s4 in s3.split(' ') if s4 != '')
                        # 括号内的部分拆开理解
                        # for s4 in s3.split(' '):
                        #    if s4!='':  这一步是去除空字符''。注意与' ' 的区别。

        # 将每个tokenized_data分别添加到tokenized_text当中
        tokenized_text.append(tokenized_data)

    return tokenized_text


# 写好函数后，我们来调用函数对英文文本分词。为了直观展示，我们自定义多个文本，模拟英文应用中的多个文本。

# In[ ]:


a = ['i am a boy?i am a boy ! i am a boy,i', 'god is a girl', 'i love you!']
tokenize_english_text(a)


# ### 中文分词

# 我们想要通过计算机将句子转化成词的表示，自动识别句子中的词，在词与词之间加入边界分隔符，分割出各个词汇。这个切词过程就叫做中文分词。

# 分词两个字的字面意思真的是十分简单，相信小学生都能理解分词的意思。但是想要计算机了解，并且实践起来却十分的复杂，困难重重。

# 由于中文的结构与印欧体系语种有很大的差异。在中文中，文本是由连续的字序列构成，词和词之间没有天然的分隔符。不同分词方法的结果影响了词性，句法等问题。分词作为一个方法，如果运用场景不同，要求不同，最终对任务达到的效果也不同。可以说中文分词相对英文分词有很大的困难。

# #### 困难之一：歧义
# 
# 原文：以前喜欢一个人，现在喜欢一个人
# 
# 这里有两个「一个人」，但是代表的意思完全不一样。

# #### 困难之一：分词界限
# 
# 原文：这杯水还没有冷
# 
# 分词一： 这 / 杯 / 水 / 还 / 没有 / 冷
# 
# 分词二： 这 / 杯 / 水 / 还没 / 有 / 冷
# 
# 分词三： 这 / 杯 / 水 / 还没有 / 冷
# 
# 这三个句子字面上是没有歧义，人类来理解也没什么差别。但是这句子到底怎么界定划分，要人类来判断划分都已经很困难了，更别说计算机来理解了。分词的界限十分难界定。可以说中文分词没有标准的分词算法，只有对应不同场景更适合的分词算法。

# 在人机自然语言交互当中，对于不同的场景，更成熟的算法更加能达到想要的效果，帮助理解复杂的中文语言。

# ### 中文方法方法

# 中文分词这个概念自提出以来，经过多年的发展，主要可以分为三个方法：机械分词方法，统计分词方法，以及两种结合起来的分词。基于统计的分词方法我们将放到下一个实验中讲解，本次实验我们将重点放在机械分词方法上。

# 机械分词方法又叫做基于规则的分词方法：这种分词方法按照一定的规则将待处理的字符串与一个词表词典中的词进行逐一匹配，若在词典中找到某个字符串，则切分，否则不切分。机械分词方法按照匹配规则的方式，又可以分为：**正向最大匹配法**，**逆向最大匹配法**和**双向匹配法**三种。

# #### 正向最大匹配法

# 正向最大匹配法（Maximum Match Method，MM 法）是指从左向右按最大原则与词典里面的词进行匹配。假设词典中最长词是 $m$ 个字，那么从待切分文本的最左边取 $m$ 个字符与词典进行匹配，如果匹配成功，则分词。如果匹配不成功，那么取 $m-1$ 个字符与词典匹配，一直取直到成功匹配为止。

# 接下来，我们用一个简单的例子来讲一下 MM 法的过程：

# - 句子： 中华民族从此站起来了 
# - 词典："中华"，"民族"，"从此"，"站起来了"

# 使用 MM 法分词：

# - 第一步：词典中最长是 4 个字，所以我们将 “中华民族” 取出来与词典进行匹配，匹配失败。
# - 第二步：于是，去掉 “族”，以 “中华民” 进行匹配，匹配失败
# - 第三步：去掉 “中华民” 中的 “民”，以 “中华” 进行匹配，匹配成功。 
# - 第四步：在带切分句子中去掉匹配成功的词，待切分句子变成 “民族从此站起来了”。
# - 第五步：重复上面的第 1 - 4 步骤
# - 第六步：若最后一个词语匹配成功，结束。

# 最终句子被分成：“中华 / 民族 / 从此 / 站起来了 ”

# #### 逆向最大匹配法

# 逆向最大匹配法（ Reverse Maximum Match Method, RMM 法）的原理与正向法基本相同，唯一不同的就是切分的方向与 MM 法相反。逆向法从文本末端开始匹配，每次用末端的最长词长度个字符进行匹配。
# 
# 因为基本原理与 MM 法一样，反向来进行匹配就行。所以这里对算法不再赘述。

# 另外，由于汉语言结构的问题，里面有许多偏正短语，即结构是：

# - 定语 + 中心词（名、代）：（祖国）大地、（一朵）茶花、（前进）的步伐。
# - 状语 + 中心词（动、形）：（很）好看、（独立）思考、（慢慢）地走。

# 因此，如果采用逆向匹配法，可以适当提高一些精确度。换句话说，使用逆向匹配法要比正向匹配法的误差要小。至于其中的原由，我们不细讲，只做了解即可。

# #### 双向最大匹配法

# 双向最大匹配法（Bi-direction Matching Method ，BMM）则是将正向匹配法得到的分词结果与逆向匹配法得到的分词结果进行比较，然后按照最大匹配原则，选取次数切分最少的作为结果。

# ### 正向最大匹配法算法实现

# 接下来，我们选择正向最大匹配法，对其进行算法实现。为了方便观察整个算法，下面是整个正向最大匹配算法的流程图：

# ![image](https://doc.shiyanlou.com/document-uid214893labid7506timestamp1542185472693.png)

# 算法步骤：

# 1. 导入分词词典  `dic`，待分词文本 `text`，创建空集 `words` 。
# 2. 遍历分词词典，找到最长词的长度，`max_len_word` 。
# 3. 将待分词文本从左向右取 `max_len=max_len_word` 个字符作为待匹配字符串 `word` 。
# 4. 将 `word` 与词典 `dic` 匹配
# 5. 若匹配失败，则 `max_len = max_len - 1` ，然后
# 6. 重复 3 - 4 步骤
# 7. 匹配成功，将 `word` 添加进 `words` 当中。
# 8. 去掉待分词文本前 `max_len` 个字符
# 9. 重置 `max_len` 值为 `max_len_word`
# 10. 重复 3 - 8 步骤
# 11. 返回列表  `words`

# 接下来，我们将在 Python 当中去实现整个 MM 法的流程。不过，为了更加直观，也为了更容易理解，这里仅创建一个简单的词典 `dic` 和短文本 `text`  。

# 首先创建一个文本 `text`：

# In[ ]:


# 文本
text = '我们是共产主义的接班人'
text


# 创建一个词典 dic：

# In[ ]:


# 词典
dic = ('我们', '是', '共产主义', '的', '接班', '人', '你', '我', '社会', '主义')
dic


# 建立一个空数组来存放分词结果：

# In[ ]:


words = []


# 我们需要遍历词典来求出最长词的长度：

# In[ ]:


# 初始最长词长度为0
max_len_word = 0

for key in dic:

    # 若当前词长度大于max_len_word，则将len(key)值赋值给max_len_word
    if len(key) > max_len_word:
        max_len_word = len(key)

max_len_word


# 下面是通过循环来完成 MM 法：

# In[ ]:


# 判断text的长度是否大于0，如果大于0则进行下面的循环
while len(text) > 0:

    # 初始化想要取的字符串长度
    # 按照最长词长度初始化
    word_len = max_len_word

    # 对每个字符串可能会有(max_len_word)次循环
    for i in range(0, max_len_word):

        # 令word 等于text的前word_len个字符
        word = text[0:word_len]

        # 为了便于观察过程，我们打印一下当前分割结果
        print('用', word, '进行匹配')

        # 判断word是否在词典dic当中

        # 如果不在词典当中
        if word not in dic:

            #则以word_len - 1
            word_len -= 1

            # 清空word
            word = []

        # 如果word 在词典当中
        else:

            # 更新text串起始位置
            text = text[word_len:]

            # 为了方便观察过程，我们打印一下当前结果
            print('{}   匹配成功，添加进words当中'.format(word))

            # 把匹配成功的word添加进上面创建好的words当中
            words.append(word)

            # 清空word
            word = []


# 显示最后的分词结果：

# In[ ]:


words


# 通过上面这段代码，我们成功用正向最大匹配法将句子分词，并返回了分词后的结果。在今后的应用中，你会遇到更长的待处理文本，必须准备更大容量的词典来应对。总的来说，这个是一个不太理想的分词方法。对词典里面没有的词几乎没有识别的能力。但是也可以应对很多的小型任务。

# ### 实验总结

# 本次实验，我们介绍了 自然语言处理 实际应用的第一步 ——分词。并介绍了中英文分词的一些基本方法。然后从机械分词方法中的正向最大匹配方法入手，完成了 Python 对中文文本的分词。下一个实验我们将介绍另一种分词方法，基于统计的分词方法。
