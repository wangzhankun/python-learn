{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "Softmax函数的本质是将一个K维的任意实数向量，压缩（映射）成另一个K维的实数向量，其中向量中的每个元素取指都介于（0,1）范围内。\n",
    "<br/>\n",
    "Softmax是对逻辑回归（Logistic Regression, LR）的推广，逻辑回归用于处理二分类问题，其推广Softmax回归则用于处理多分类问题。如下图示，在数学上，Softmax函数会返回输出类的互斥概率分布，例如，网络的输出为（1,1,1,1），经过Softmax函数后输出为（0.25,0.25,0.25,0.25）。我们可以得到分类中唯一所属类别，因此通常把Softmax作为输出层的激活函数。\n",
    "$$\n",
    "softmax(x) = \\frac{e^{x_j}}{\\sum_{k=1}^{K}e^{x_k}}\n",
    "$$\n",
    "\n",
    "下面进一步举例说明。假设有一个多分类问题，但是我们只关心这些类别的最高得分的概率，那么会使用一个带有最大似然估计函数的Softmax输出层来获得所有类比输出概率的最大值。例如神经网络的分类有3个，分别为“野马”“河马”“斑马”，使用softmax作为输出层的激活函数最后只能得到一个最大的分类概率如野马（0.6），河马（0.1），斑马（0.3），其中最大值野马（0.6）。\n",
    "<br/>\n",
    "如果要求每次的输出都可以获得多个分类，例如希望神经网络的预测输出既像“河马”也像“野马”，那么我们不希望Softmax作为输出层，这里可以使用Sigmoid函数作为输出层的激活函数更合适，因为Sigmoid函数可以为每个类别的输出提供独立的概率。\n",
    "<br/>\n",
    "### 性质\n",
    "$$\n",
    "softmax(x) = softmax(x+c),其中c为实数\n",
    "$$\n",
    "Reference:\n",
    "* [Softmax Function wiki](https://en.wikipedia.org/wiki/Softmax_function)\n",
    "* [Classification and Loss Evaluation - Softmax and Cross Entropy Loss](https://deepnotes.io/softmax-crossentropy)\n",
    "* [Word2Vec介绍：softmax函数的python实现](https://zhuanlan.zhihu.com/p/28991249)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    对输入x的每一行计算softmax\n",
    "    参数：\n",
    "    x: 矩阵\n",
    "    返回值：\n",
    "    x: 对应矩阵\n",
    "    \"\"\"\n",
    "    origin_shape = x.shape\n",
    "    \n",
    "    if len(x.shape) > 1:\n",
    "        # 矩阵\n",
    "        tmp = np.max(x, axis=1)\n",
    "        x -= tmp.reshape((x.shape[0], 1))\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x, axis = 1) # 每行求和\n",
    "        x /= tmp.reshape((x.shape[0]), 1)\n",
    "        \n",
    "    else:\n",
    "        tmp = np.max(x)\n",
    "        x -= tmp\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x)\n",
    "        x /= tmp\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Gram\n",
    "## 什么是skip gram\n",
    "就是在已知目标单词的情况下，预测它的上下文单词。\n",
    "<br/>\n",
    "## 目标\n",
    "计算在给定单词条件下，其他单词出现的概率。\n",
    "## 词向量表示\n",
    "### one-hot\n",
    "one-hot就是利用$R^{|V| \\times 1}$向量表示单词。其中$|V|$是单词数量。\n",
    "<br/>\n",
    "我们用$w_c$表示目标单词的one-hot向量。\n",
    "<br/>\n",
    "### word embedding\n",
    "例如：\n",
    "$$\n",
    "v_c = \\left[\n",
    "        \\begin{matrix}\n",
    "        0.2 \\\\\n",
    "        0.5  \\\\\n",
    "        ...  \\\\\n",
    "        0.1\n",
    "        \\end{matrix}\n",
    "        \\right]\n",
    "$$\n",
    "这是一个词向量表示。其中我们用$v_c$示目标单词的词向量。\n",
    "### 单词矩阵\n",
    "单词矩阵就是所有单词的词向量集合。\n",
    "<br/>\n",
    "主要，我们这里需要用到两个单词矩阵，一个是目标单词组成的矩阵$W \\in R^{d \\times V}$。另外一个矩阵由除了目标单词外的其他单词的**词向量的转置**组成的矩阵，用$W' \\in R^{V \\times d}$。\n",
    "<br/>\n",
    "另外需要说明的是，由于每一个单词都有可能作为目标单词或其他单词，因此，实际上这两个矩阵是分别包含所有单词的词向量的。\n",
    "<br/>\n",
    "### 单词相似度\n",
    "两个单词求内积。\n",
    "<br/>\n",
    "### Softmax函数\n",
    "我们需要知道的是softmax函数就是能够把输入转换为概率分布，也就是说使输入的实数变成分数。除此之外的内容我们暂不讨论。\n",
    "$$\n",
    "\\sigma(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^{K}e^{z_k}}\n",
    "\\\\\n",
    "z = u^T_xv_c\n",
    "$$\n",
    "\n",
    "## 总结\n",
    "\n",
    "$$\n",
    "w_c : 目标单词one-hot向量\n",
    "\\\\\n",
    "v_c ： 目标单词词向量\n",
    "\\\\\n",
    "u_x : 表示除目标单词外第x个单词的词向量\n",
    "\\\\\n",
    "W \\in R^{d \\times V} : 目标单词矩阵\n",
    "\\\\\n",
    "W' \\in R^{V \\times d} : 其他单词矩阵\n",
    "\\\\\n",
    "W' = W^T\n",
    "\\\\\n",
    "d : 词向量维度\n",
    "\\\\\n",
    "V : 词汇表维度\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 理解算法过程\n",
    "求相似度$u^T_xv_c$步骤：\n",
    "1. 求$v_c$\n",
    "   $$\n",
    "   v_c = Ww_c\n",
    "   \\\\\n",
    "    W \\in R^{d \\times V}\n",
    "    \\\\\n",
    "    w_c \\in R^{V \\times 1}\n",
    "    v_c \\in R^{d \\times 1}\n",
    "   $$\n",
    "   求$v_c$就是在$W$中进行索引的过程，在实际操作中，采用索引进行。\n",
    "2. 求$u^T_xv_c$\n",
    "    <br/>\n",
    "    相似度可有矩阵$W'v_c$求得。$u^T_xv_c$就是结果的每一行向量\n",
    "3. 求softmax\n",
    "    <br/>\n",
    "    这步比较简单，把得到的相似度矩阵代入softmax公式，就得到了一个满足概率分布的矩阵。\n",
    "    \n",
    "\n",
    "至此，我们实现了我们的目标：得到一个向量。\n",
    "$$\n",
    "W' v_c\n",
    "\n",
    "=\n",
    "\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "u_1^Tv_c\n",
    "\\\\\n",
    "u_2^Tv_c\n",
    "\\\\\n",
    "u_3^Tv_c\n",
    "\\\\\n",
    "...\n",
    "\\\\\n",
    "u_V^Tv_c\n",
    "\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\n",
    "= \n",
    "\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "P(u_1 | v_c)\n",
    "\\\\\n",
    "P(u_2 | v_c)\n",
    "\\\\\n",
    "P(u_3 | v_c)\n",
    "\\\\\n",
    "...\n",
    "\\\\\n",
    "P(u_V | v_c)\n",
    "\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
