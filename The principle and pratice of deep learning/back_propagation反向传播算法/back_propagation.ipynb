{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反向传播算法\n",
    "这里的神经网络是有3层（包括输入层），输入层有3个神经元，隐层有4个神经元，输出层有2个神经元<br/>\n",
    "根据网络模型的大小，利用高斯分布函数的权重参数 weights 和偏置参数 biases 产生均值为0、方差为1的随机值</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义神经网络模型架构[input, hidden layer, output],定义每层的神经元数量\n",
    "import numpy as np \n",
    "network_sizes = [3,4,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.35597066, -1.01841344,  2.59380614, -0.73690691],\n",
      "       [ 2.44892132, -1.25187742, -1.76287532, -1.15855486],\n",
      "       [ 0.69931761, -0.55139777, -1.53829358, -0.46352428]]), array([[-1.08036395, -0.94780111],\n",
      "       [ 0.30961408,  1.18799913],\n",
      "       [-0.74503102,  0.21669354],\n",
      "       [-2.25923162, -1.45838943]])]\n",
      "[array([[ 0.98020682],\n",
      "       [-0.95937384],\n",
      "       [-0.44982774]]), array([[-0.32846922],\n",
      "       [ 0.09661282],\n",
      "       [ 0.52227908],\n",
      "       [-0.3661237 ]]), array([[0.09770412],\n",
      "       [0.15838912]])]\n"
     ]
    }
   ],
   "source": [
    "# 初始化神经网络参数\n",
    "sizes = network_sizes\n",
    "# 网络层数\n",
    "num_layers = len(sizes)\n",
    "# 生成h*1的矩阵\n",
    "biases = [np.random.randn(h, 1) for h in sizes]\n",
    "# 生成3*4和4*2的矩阵\n",
    "weights = [np.random.randn(x,y) for x,y in zip(sizes[:-1], sizes[1:])]\n",
    "print(weights)\n",
    "print(biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "$$\n",
    "L = \\frac{1}{2}(network\\_y - real\\_y)^{2}  \\\\\n",
    "\\frac{dL}{dnetwork\\_y} = network\\_y - real\\_y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_der(network_y, real_y):\n",
    "    \"\"\"\n",
    "    返回损失函数的偏导，损失函数使用 MSE\n",
    "    L = 1 / 2 * (network_y - real_y) ^ 2\n",
    "    delta_L = network_y - real_y\n",
    "    \"\"\"\n",
    "    return (network_y - real_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数\n",
    "激活函数使用sigmod</br>\n",
    "$$\n",
    "s(x) = \\frac{1}{1+e^{-x}}    \\\\\n",
    "s^{'}(x) = \\frac{e^{-x}}{(1+e^{-x})^{2}}=s(x)(1-s(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活函数\n",
    "def sigmoid(z):\n",
    "    \"\"\"激活函数使用 sigmoid \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "# 激活函数的偏导\n",
    "def sigmoid_der(z):\n",
    "    \"\"\"sigmoid 函数的导数 \"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播算法的具体实现\n",
    "backprop() 函数的输入为 x,y ，根据反向传播的四个基本公式的计算中需要知道每一层神经元的激活值和加权输入值，因此在进行向前传播时，分别使用activations记录每一层的激活值和zs记录每一层的加权输入值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(x,y):\n",
    "    \"\"\"反向传播算法的实现\"\"\"\n",
    "\n",
    "    # 1) 初始化网络参数的导数  权重w的偏导和偏置b的偏导\n",
    "    delta_w = [np.zeros(w.shape) for w in weights]\n",
    "    delta_b = [np.zeros(b.shape) for b in biases]\n",
    "\n",
    "    # 2) 前向传播 feed forward\n",
    "    activation = x # 把输入的数据作为第一次激活值\n",
    "    activations = [x] # 存储网络的激活值\n",
    "    zs = [] # 存储网络的加权输入值 (z = wx + b)， 注意没有记录input\n",
    "\n",
    "    for w,b in zip(weights,biases):\n",
    "        z = np.dot(w,activation) + b\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation) # 记录激活值\n",
    "        zs.append(z) # 记录加权输入\n",
    "\n",
    "\n",
    "    # 3）反向传播\n",
    "    # bp1 计算传输层误差\n",
    "    delta_L = loss_der(activations[-1],y) * sigmoid_der(zs[-1])\n",
    "    \n",
    "    # bp3 计算输出层关于偏置的误差\n",
    "    delta_b[-1] = delta_L\n",
    "\n",
    "    # bp4 损失函数在输出层关于权值的偏导\n",
    "    delta_w[-1] = np.dot(delta_L,activations[-2].transpose())\n",
    "\n",
    "    delta_l = delta_L\n",
    "    for l in range(2,num_layers):\n",
    "        # bp2 计算第一层误差\n",
    "        z = zs[-l]\n",
    "        sp = sigmoid_der(z)\n",
    "        print(weights[-l+1].transpose())\n",
    "        print(delta_l)\n",
    "        print(sp)\n",
    "        delta_l = np.dot(weights[-l+1].transpose(), delta_l) * sp\n",
    "        # bp3 损失函数在l层关于偏置的偏导\n",
    "        delta_b[-l] = delta_l\n",
    "        # bp4 损失函数在l层关于权值的偏导\n",
    "        delta_w[-l] = np.dot(delta_l,activations[-l-1].transpose())\n",
    "\n",
    "    return (delta_w,delta_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机产生训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[0.32192011]\n",
      " [0.87851601]\n",
      " [0.45994267]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "training_x = np.random.rand(3).reshape(3,1)\n",
    "print(type(training_x))\n",
    "print(training_x)\n",
    "training_y = np.array([0,1]).reshape(2,1)\n",
    "print(type(training_y))\n",
    "print(training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,4) and (3,1) not aligned: 4 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f3bd3d5c0368>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-3b632f8b2941>\u001b[0m in \u001b[0;36mbackprop\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 记录激活值\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (3,4) and (3,1) not aligned: 4 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "backprop(training_x, training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"weights:\\n{}\".format(weights))\n",
    "print(\"biases:\\n{}\".format(biases))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
