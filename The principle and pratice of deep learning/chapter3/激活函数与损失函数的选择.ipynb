{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ReLU + MSE\n",
    "均方误差损失函数无法处理梯度消失的问题，而是用Leaky ReLU激活函数能够减少计算时梯度消失的问题，因此在神经网络中如果需要使用均方误差损失函数，一般采用Leaky ReLU等可以减少梯度消失的激活函数。\n",
    "<br/>\n",
    "由于MSE具有普遍性，一般作为衡量损失值的标准，因此使用MSE作为损失函数表现既不会太好，也不会太差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sigmoid + Logistic\n",
    "Sigmoid损失函数会引起梯度消失问题：根据链式法则，Sigmoid函数求导后由多个\\[0,1\\]范围的数进行连乘，如其导数形式为$s(x)(1-s(x))$，当其中一个数很小时，连乘后会无限趋近于0,。\n",
    "<br/>\n",
    "类Logistic损失函数求导时，加上对数后连乘操作转化为求和，在一定程度上避免了梯度消失。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Softmax + Logistic\n",
    "Softmax激活函数会返回输出类的互斥概率分布，也就是能把离散的输出转换为一个同分布互斥的概率。\n",
    "<br/>\n",
    "类Logistic损失函数是基于概率的最大似然估计函数计算而来的因此输出概率化能够更加方便优化算法进行求导和计算。所以我们经常看到的输出层就是Softmax+交叉熵损失函数"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
